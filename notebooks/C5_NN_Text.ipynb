{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация текстовых документов посредством нейронных сетей\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "С.Ю. Папулин (papulin.study@yandex.ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Содержание\n",
    "\n",
    "- [Предобработка данных](#Предобработка-данных)\n",
    "- [Классификация текстовых документов](#Классификация-текстовых-документов)\n",
    "    - [Загрузка исходных данных](#Загрузка-исходных-данных)\n",
    "    - [Наивный байесовский классификатор](#Наивный-байесовский-классификатор)\n",
    "    - [Полносвязная нейронная сеть](#Полносвязная-нейронная-сеть)\n",
    "- [Рекуррентная нейронная сеть](#Рекуррентная-нейронная-сеть)\n",
    "    - [LSTM](#LSTM)\n",
    "    - [Embedding](#Embedding)\n",
    "    - [Embedding и LSTM](#Embedding-и-LSTM)\n",
    "    - [Сверточная нейронная сеть](#Сверточная-нейронная-сеть)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подключение библиотек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Команды установки `TensorFlow`:\n",
    "```bash\n",
    "pip install --upgrade pip\n",
    "pip install tensorflow==2.10.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подключение пакетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade tensorflow-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обработка естественного языка с `TextVectorization`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обработка структурированных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слои для обработки категориальных признаков:\n",
    "- `StringLookup` \n",
    "- `IntegerLookup`\n",
    "- `Hashing` \n",
    "- `CategoryEncoding`\n",
    "\n",
    "Слои для обработки числовых призгаков:\n",
    "- `Normalization`\n",
    "- `Discretization`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обработка изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стандартизация:\n",
    "- `Resizing`\n",
    "- `CenterCrop`\n",
    "- `Rescaling`\n",
    "\n",
    "Аугментация:\n",
    "- `RandomCrop`\n",
    "- `RandomFlip`\n",
    "- `RandomTranslation`\n",
    "- `RandomZoom`\n",
    "- `RandomRotation`\n",
    "- `RandomHeight`\n",
    "- `RandomWidth`\n",
    "- `RandomContrast`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация текстовых документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка исходных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(\n",
    "    subset=\"all\", \n",
    "    shuffle=True, \n",
    "    remove=(\"headers\", \"footers\", \"quotes\"), \n",
    "    random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = np.array(data.target_names)\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = (names=='comp.graphics')\\\n",
    "    | (names=='comp.os.ms-windows.misc')\\\n",
    "    | (names=='comp.sys.ibm.pc.hardware')\\\n",
    "    | (names=='comp.sys.mac.hardware')\\\n",
    "    | (names=='comp.windows.x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = np.where(topics)[0]\n",
    "topic_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.where(np.isin(data.target, topic_labels), 1, 0)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Формирование тестового множества\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.data, y, test_size=0.3, random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Формирование проверочного множества\n",
    "X_train_, X_val, y_train_, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.3, random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наивный байесовский классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=True, stop_words=\"english\",\n",
    "                             use_idf=False, ngram_range=(1,1),\n",
    "                             max_features=NUM_FEATURES,\n",
    "                             smooth_idf=True)                        \n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(vectorizer.vocabulary_.keys())\n",
    "vocab[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = vectorizer.build_tokenizer()\n",
    "analyzer = vectorizer.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Средняя длина текста\n",
    "np.mean(list(map(lambda x: len(tokenizer(x)), X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Средняя длина текста с учетом словаря\n",
    "np.mean(list(map(lambda x: len(analyzer(x)), X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание и обучение модели\n",
    "m_multNB = MultinomialNB(alpha=0.1).fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Предсказания для тестового подмножества\n",
    "# y_test_pred = m_multNB.predict(X_val_tfidf)\n",
    "\n",
    "# Доля правильных классификаций на тестовом подмножестве\n",
    "print(\"Accuracy =\", m_multNB.score(X_test_tfidf, y_test))\n",
    "print(\"Precision =\", precision_score(m_multNB.predict(X_test_tfidf), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Полносвязная нейронная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_layer = layers.TextVectorization(\n",
    "    output_mode=\"tf_idf\",\n",
    "    max_tokens=NUM_FEATURES\n",
    ")\n",
    "\n",
    "vectorizer_layer.adapt(X_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_layer.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(vectorizer_layer)\n",
    "    model.add(layers.Dense(128, activation=\"relu\", input_shape=(NUM_FEATURES,)))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.Precision()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение модели\n",
    "model = build_model()\n",
    "\n",
    "# Описание модели\n",
    "model.summary()\n",
    "\n",
    "# Обучение\n",
    "train_history = model.fit(\n",
    "    X_train_, y_train_.tolist(), \n",
    "    epochs=15, \n",
    "    validation_data=(X_val, y_val.tolist()),\n",
    "    batch_size=50,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDX = 0\n",
    "\n",
    "# Построение графиков ошибок обучения\n",
    "plt.figure(figsize=[14, 4])\n",
    "\n",
    "epochs = np.arange(1, len(train_history.history[\"loss\"])+1)\n",
    "\n",
    "plt.subplot(1,2,1)  # кросс-энтропия\n",
    "plt.plot(epochs[INDX:], train_history.history[\"loss\"][INDX:], \"-og\", label=\"train\")\n",
    "plt.plot(epochs[INDX:], train_history.history[\"val_loss\"][INDX:], \"-o\", color=\"orange\", label=\"val\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)  # доля правильных классификаций\n",
    "plt.plot(epochs[INDX:], train_history.history[\"binary_accuracy\"][INDX:], \"-og\", label=\"train\")\n",
    "plt.plot(epochs[INDX:], train_history.history[\"val_binary_accuracy\"][INDX:], \"-o\", color=\"orange\", label=\"val\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбираем количество эпох и заново обучаем сеть на всём обучающем множестве\n",
    "best_num_epochs = 5\n",
    "\n",
    "# Построение модели\n",
    "model = build_model()\n",
    "\n",
    "# Обучение\n",
    "train_history = model.fit(X_train, y_train.tolist(),\n",
    "                          epochs=best_num_epochs, \n",
    "                          batch_size=50,\n",
    "                          verbose=1)\n",
    "\n",
    "# Оценка качества модели\n",
    "_, train_error__acc, train_error__prec = model.evaluate(X_train, y_train.tolist())\n",
    "_, test_error__acc, test_error__prec = model.evaluate(X_test, y_test.tolist())\n",
    "\n",
    "print(\"Train:\\n\")\n",
    "print(\"\\tAccuracy = \\t\", train_error__acc)\n",
    "print(\"\\tPrecision = \\t\", train_error__prec)\n",
    "print(\"Test:\\n\")\n",
    "print(\"\\tAccuracy = \\t\", test_error__acc)\n",
    "print(\"\\tPrecision = \\t\", test_error__prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Рекуррентная нейронная сеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразование текстовых документов в векторный вид"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = 10000\n",
    "TEXT_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование текста в последовательность индексов слов\n",
    "# одной длины с добавлением нулей, если количество токенов \n",
    "# меньше выборанного значения\n",
    "vectorizer_layer = layers.TextVectorization(\n",
    "    output_sequence_length=TEXT_LENGTH,\n",
    "    output_mode=\"int\",\n",
    "    max_tokens=NUM_FEATURES,\n",
    "    standardize=\"lower_and_strip_punctuation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_layer.adapt(X_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_layer.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_layer.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokens_padded = vectorizer_layer(X_train_)\n",
    "X_val_tokens_padded = vectorizer_layer(X_val)\n",
    "X_train_tokens_padded[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ENCODER_FEATURES = vectorizer_layer.vocabulary_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot преобразование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В рекуррентную сеть необходимо подавать токены в виде вектора. Простейший вариант это представить токен как вектор размера `NUM_FEATURES` (размер словаря), в котором элемент с индексом токена равен  `1`, а все остальные `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x):\n",
    "    # tranform text to sequence of indices\n",
    "    outputs_seq_ = tf.reshape(vectorizer_layer([x,])[0], shape=(-1,1))\n",
    "    features_indx = tf.reshape(tf.range(0, TEXT_LENGTH, dtype=\"int64\"), shape=(-1,1))\n",
    "    index_to_update = tf.concat((features_indx, outputs_seq_), axis=1)\n",
    "    # one-hot encoding\n",
    "    outputs_one_hot = tf.zeros((TEXT_LENGTH, NUM_ENCODER_FEATURES))\n",
    "    outputs_one_hot = tf.tensor_scatter_nd_update(outputs_one_hot, index_to_update, tf.ones(TEXT_LENGTH))\n",
    "    # remove 1's from the 1st column\n",
    "    first_index = tf.reshape(tf.zeros(TEXT_LENGTH, dtype=\"int64\"), shape=(-1,1))\n",
    "    index_to_remove = tf.concat((features_indx, first_index), axis=1)\n",
    "    return tf.tensor_scatter_nd_update(outputs_one_hot, index_to_remove, tf.zeros(TEXT_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot(X_train_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_dataset = tf.data.Dataset.from_tensor_slices(X_train_).map(one_hot)\n",
    "train_y_dataset = tf.data.Dataset.from_tensor_slices(y_train_)\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.zip((train_X_dataset, train_y_dataset))\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_X_dataset = tf.data.Dataset.from_tensor_slices(X_val).map(one_hot)\n",
    "val_y_dataset = tf.data.Dataset.from_tensor_slices(y_val)\n",
    "val_dataset = (\n",
    "    tf.data.Dataset.zip((val_X_dataset, val_y_dataset))\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Masking(mask_value=0., input_shape=(TEXT_LENGTH, NUM_ENCODER_FEATURES)))\n",
    "    model.add(layers.LSTM(units=64, input_shape=(TEXT_LENGTH, NUM_ENCODER_FEATURES)))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.Precision()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение модели\n",
    "model = build_model()\n",
    "\n",
    "# Описание модели\n",
    "model.summary()\n",
    "\n",
    "# Обучение\n",
    "train_history = model.fit(\n",
    "    train_dataset, \n",
    "    epochs=5, \n",
    "    batch_size=100,\n",
    "    validation_data=val_dataset,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_dataset = tf.data.Dataset.from_tensor_slices(X_train_)\n",
    "train_y_dataset = tf.data.Dataset.from_tensor_slices(y_train_)\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.zip((train_X_dataset, train_y_dataset))\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_X_dataset = tf.data.Dataset.from_tensor_slices(X_val)\n",
    "val_y_dataset = tf.data.Dataset.from_tensor_slices(y_val)\n",
    "val_dataset = (\n",
    "    tf.data.Dataset.zip((val_X_dataset, val_y_dataset))\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(vectorizer_layer)\n",
    "    model.add(layers.Embedding(\n",
    "        input_length=TEXT_LENGTH,\n",
    "        input_dim=NUM_ENCODER_FEATURES,\n",
    "        output_dim=128,\n",
    "        mask_zero=True))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.Precision()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение модели\n",
    "model = build_model()\n",
    "\n",
    "# Описание модели\n",
    "model.summary()\n",
    "\n",
    "# Обучение\n",
    "train_history = model.fit(\n",
    "    train_dataset, \n",
    "    epochs=10, \n",
    "    batch_size=100,\n",
    "    validation_data=val_dataset,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding и LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(vectorizer_layer)\n",
    "    model.add(layers.Embedding(\n",
    "        input_dim=NUM_ENCODER_FEATURES,\n",
    "        input_length=TEXT_LENGTH,\n",
    "        output_dim=128,\n",
    "        mask_zero=True))\n",
    "    model.add(layers.Bidirectional(layers.LSTM(32)))\n",
    "    model.add(layers.Dense(32, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1,  activation=\"sigmoid\"))\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.Precision()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение модели\n",
    "model = build_model()\n",
    "\n",
    "# Описание модели\n",
    "model.summary()\n",
    "\n",
    "# Обучение\n",
    "train_history = model.fit(\n",
    "    train_dataset, \n",
    "    epochs=10, \n",
    "    batch_size=50,\n",
    "    validation_data=val_dataset,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сверточная нейронная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(vectorizer_layer)\n",
    "    model.add(layers.Embedding(\n",
    "        input_length=TEXT_LENGTH,\n",
    "        input_dim=NUM_ENCODER_FEATURES,\n",
    "        output_dim=256,\n",
    "        mask_zero=True))\n",
    "    model.add(layers.Conv1D(128, kernel_size=5, activation=\"relu\"))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(32, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.Precision()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение модели\n",
    "model = build_model()\n",
    "\n",
    "# Описание модели\n",
    "model.summary()\n",
    "\n",
    "# Обучение\n",
    "train_history = model.fit(\n",
    "    train_dataset, \n",
    "    epochs=10, \n",
    "    batch_size=100,\n",
    "    validation_data=val_dataset,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
