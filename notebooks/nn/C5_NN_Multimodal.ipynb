{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f27a42e-1cfa-4a78-91a6-12bb1d86f709",
   "metadata": {},
   "source": [
    "# Multimodal Model\n",
    "---\n",
    "\n",
    "S.Yu. Papulin (papulin.study@yandex.ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bc153f-5f56-4a63-bc79-af94825bb742",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "- [Loading Dataset](#Loading-Dataset)\n",
    "- [Image Encoder](#Image-Encoder)\n",
    "- [Text Encoder](#Text-Encoder)\n",
    "- [Multimodal Model](#Multimodal-Model)\n",
    "- [Pretrained `CLIP` model](#Pretrained-CLIP-model)\n",
    "- [Sources](#Sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e098c-76d9-460b-aaad-649cb84aa549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import (\n",
    "    layers, \n",
    "    models, \n",
    "    Model, \n",
    "    utils, \n",
    "    losses, \n",
    "    optimizers, \n",
    "    metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28457c72-0a67-49ae-af4f-53c6c90c3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc999bc5-ffef-4e80-9796-e8b93b887215",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4836c2-04ab-42c8-b51a-42ed863a75a6",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e898675b-6b70-4374-9448-c846f112b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and show shape of data\n",
    "(X_trainval, y_trainval), (X_test, y_test) = cifar10.load_data()\n",
    "X_trainval.shape, y_trainval.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bff2c8-38d2-4d9e-a6b1-664840d2dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image value range\n",
    "X_trainval.max(), X_trainval.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec830be2-0db9-4d79-822b-42c5d1307de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique targets and their counts\n",
    "np.unique(y_trainval, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66e8565-7a89-4908-a908-1c4d6945e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First n targets\n",
    "y_trainval[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982c99bd-8bdb-481c-92d7-afc077d57521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class labels\n",
    "labels = np.array([\n",
    "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
    "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e41cc72-d9f1-4c7e-8931-35ec0c11e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(labels)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb44c45a-a2a4-4413-af2c-ff5397028d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 10 random images of each class\n",
    "NUM_DISPLAY_IMAGES = 10\n",
    "for target in range(num_classes):\n",
    "    indices = np.asarray(y_trainval==target).nonzero()[0]\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    indices_rnd = np.random.choice(indices, NUM_DISPLAY_IMAGES, replace=False)\n",
    "    print(f'Class label: {labels[target]}')\n",
    "    plt.figure(figsize=[10, 10])\n",
    "    for i in range(NUM_DISPLAY_IMAGES):\n",
    "        plt.subplot(1, NUM_DISPLAY_IMAGES, i+1)\n",
    "        plt.title(indices_rnd[i])\n",
    "        plt.imshow(X_trainval[indices_rnd[i]])\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57814b52-2a8b-43d9-a2a0-5c6dd2fab252",
   "metadata": {},
   "source": [
    "## Image Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c0a70c-e213-4a1d-8e1c-dd3814563111",
   "metadata": {},
   "source": [
    "Image encoder represents our image as a vector in some multidimensional space. This vector should contain semantic information about the image. In our case, we have the image classifier. The last hidden layer is a classification layer with 10 neurons for each class. We can get rid of the last layer. Another option is to keep the last convolutional layer and average the output of all filters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628feca9-8c63-4f62-8d3a-4f4a96218743",
   "metadata": {},
   "source": [
    "### Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375c2700-4201-4972-a862-d75ea788e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose train and validation subsets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, \n",
    "    y_trainval, \n",
    "    test_size=0.1, \n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3783106a-90e7-48fe-aadd-11c8b67e77c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753faa11-6fb3-4dfe-a238-339d3cfe947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tf_dataset(X, y, batch_size=64, use_one_hot=False):\n",
    "    X = X.astype('float32') / 255.0\n",
    "    if use_one_hot:\n",
    "        y = utils.to_categorical(y)\n",
    "    else:\n",
    "        y = y.flatten()\n",
    "    return (\n",
    "        tf.data.Dataset.from_tensor_slices((X, y))\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "\n",
    "def print_first_batch(ds):\n",
    "    for X_batch, y_batch in ds.take(1):\n",
    "        print(X_batch)\n",
    "        print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4699c6-27b5-4327-9bc3-9d674c0a85ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgrecog_train_ds = convert_to_tf_dataset(X_train, y_train)\n",
    "imgrecog_val_ds = convert_to_tf_dataset(X_val, y_val)\n",
    "imgrecog_test_ds = convert_to_tf_dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1a314f-4571-472d-9908-e28e3fb68b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_first_batch(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481fe5e9-2bea-4336-8265-aacc477ff954",
   "metadata": {},
   "source": [
    "### Image recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d3a4b3-4b1c-4b4d-b87a-9b90b2122e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_tiny_conv_model(model_name='tiny_conv_net_128@10.keras'):\n",
    "    \"\"\"\n",
    "    Note: From the C5_NN_ImageRecognition notebook\n",
    "    \n",
    "    If you use TinyConvModel, you need to have access to \n",
    "    the implementation of this class.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    BASE_MODEL_PATH = '~/.keras/models'\n",
    "    model_filename = model_name\n",
    "    model_path = os.path.expanduser(os.path.join(BASE_MODEL_PATH, model_filename))\n",
    "    return models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83cbf38-ecd0-45c2-918c-3d1190513b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tiny_conv_model():\n",
    "    model = models.Sequential(name=\"ConvNet\")\n",
    "    model.add(layers.Input(shape=(32, 32, 3)))\n",
    "    model.add(layers.Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\", name=\"layer_1\"))\n",
    "    model.add(layers.MaxPooling2D((2, 2), name=\"transform_1\"))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\", name=\"layer_2\"))\n",
    "    model.add(layers.Dropout(0.1, name=\"dropout\"))\n",
    "    model.add(layers.Flatten(name=\"transform_2\"))\n",
    "    model.add(layers.Dense(128, activation=\"relu\", name=\"layer_3\"))\n",
    "    model.add(layers.Dense(10, name=\"layer_4\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b3aae0-094b-4a22-a40c-7cb3f4fbbf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgrecog_model = build_tiny_conv_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a9aae-7ae4-477c-951a-94a8f893cba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgrecog_model = build_tiny_conv_model()\n",
    "imgrecog_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-3), \n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[metrics.SparseCategoricalAccuracy(),]\n",
    ")\n",
    "imgrecog_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0bd890-eed6-40f9-b1e6-ccb2e80ad788",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "\n",
    "train_history = imgrecog_model.fit(\n",
    "    imgrecog_train_ds,\n",
    "    # validation_split=0.1,\n",
    "    validation_data=imgrecog_val_ds,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e1e6d-5842-4322-96a1-6c7f9508998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_error = imgrecog_model.evaluate(imgrecog_test_ds)\n",
    "test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefa5d0b-4852-44a5-b246-ca7ef282d527",
   "metadata": {},
   "source": [
    "### Building image encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e5429-24ab-4039-aa4d-73973eac134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers of classifier\n",
    "imgrecog_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badee109-45b4-4d40-95db-e84ca1511b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_image_encoder_on_tiny_conv():\n",
    "    \"\"\"Build a model containing all layers except the last one.\"\"\"\n",
    "    return Model(\n",
    "        inputs=imgrecog_model.inputs,\n",
    "        outputs=imgrecog_model.layers[-2].output,\n",
    "        trainable=False\n",
    "    )\n",
    "\n",
    "\n",
    "def build_image_encoder_on_resnet():\n",
    "    \"\"\"More advanced image encoder based on the ResNet50 model.\"\"\"\n",
    "    return tf.keras.applications.ResNet50(\n",
    "        # exclude last layer\n",
    "        include_top=False, \n",
    "        # average across all filters\n",
    "        pooling=\"avg\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9906e0-09a4-414d-aca6-940f3da887c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder = build_image_encoder_on_tiny_conv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad2bb6d-c70b-4eee-b37e-b88c236ecf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75864d0d-bffc-47f9-9314-17be79b30356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can freeze model by making weights non-trainable.\n",
    "# In this example we leave the last layer trainable\n",
    "image_encoder.trainable = True \n",
    "\n",
    "for layer in image_encoder.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "image_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6b8317-c223-4d53-a22b-acb01d3e36c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide some image as input to check encoder output\n",
    "image_embeddings = image_encoder(X_test[:10])\n",
    "image_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0455c01b-441b-4e33-97ec-44569e01e7b9",
   "metadata": {},
   "source": [
    "## Text Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659b3603-d6e1-42b3-9fd1-e04726cf74ed",
   "metadata": {},
   "source": [
    "Similarly, text encoder represents any text as a vector in some multidimensional space. For our text encoder we take pretained embedding model, that we discussed earlier (`glove`). Based on this model, we build a sequential model with an average over all vectors of words in the provided text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cee311-729d-4435-b964-b9471df1a716",
   "metadata": {},
   "source": [
    "### Loading embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3861a1a9-a251-4e42-84a9-8ddc082a367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(path_to_file):\n",
    "    \"\"\"Load words and their weights from file.\"\"\"\n",
    "    words = list()\n",
    "    embeddings = list()\n",
    "    with open(path_to_file) as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "            words.append(word)\n",
    "            embeddings.append(coefs)\n",
    "    return np.array(words), np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88959735-eaf9-4a0e-839e-f8e0524c283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "FILEPATH = f'/media/sf_practice/data/debug_glove/glove.6B/glove.6B.{EMBEDDING_DIM}d.txt'\n",
    "\n",
    "# Load words and their embeddings\n",
    "words, embeddings = load_vectors(FILEPATH)\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8ec9b-f03b-412d-96d6-09ade50fa08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcf58d0-721d-4a77-a3bb-732bd4f74cda",
   "metadata": {},
   "source": [
    "### Building vectorizer layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a04ae-ab26-4cd4-a619-f4e31c4a0d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TEXT_LENGTH = 20\n",
    "NUM_FEATURES = len(words) + 2\n",
    "\n",
    "\n",
    "def build_vectorizer_layer():\n",
    "    # setup vectorizer layer\n",
    "    vectorizer_layer = layers.TextVectorization(\n",
    "        max_tokens=NUM_FEATURES, \n",
    "        output_sequence_length=MAX_TEXT_LENGTH,\n",
    "        output_mode=\"int\"\n",
    "    )\n",
    "    # build vocabulary\n",
    "    vectorizer_layer.adapt(words)\n",
    "    return vectorizer_layer\n",
    "\n",
    "\n",
    "def build_vectorizer_layer_alt():\n",
    "    # setup vectorizer layer\n",
    "    vectorizer_layer = layers.TextVectorization(\n",
    "        max_tokens=NUM_FEATURES, \n",
    "        output_sequence_length=MAX_TEXT_LENGTH,\n",
    "        output_mode=\"int\"\n",
    "    )\n",
    "    # set vocabulary\n",
    "    vectorizer_layer.set_vocabulary(words)\n",
    "    return vectorizer_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d58f27-4d52-4558-a7fa-73d15acd5b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_layer = build_vectorizer_layer_alt()\n",
    "print(f'Number of tokens:\\t{len(vectorizer_layer.get_vocabulary())}')\n",
    "print(f'First few tokens:\\t{vectorizer_layer.get_vocabulary()[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea4115c-eeb0-4e10-bb66-050d505d3523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to token ids\n",
    "token_ids = vectorizer_layer('A photo of a cat').numpy()\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16d6437-cf54-4208-8965-89a42da92c0a",
   "metadata": {},
   "source": [
    "### Building embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16f9385-ecf5-4a80-a04b-92ad66bae37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 2 extra tokens: padding and [UNK]\n",
    "E = np.zeros((NUM_FEATURES, EMBEDDING_DIM))\n",
    "# E[1] = np.random.normal(0, 0.1, EMBEDDING_DIM) # [UNK]\n",
    "E[2:] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a465617-8d9d-4949-9d57-720715c2d389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_layer():\n",
    "    # setup embedding layer\n",
    "    embedding_layer = layers.Embedding(\n",
    "        input_dim=NUM_FEATURES,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        trainable=False  # disable training\n",
    "    )\n",
    "    # initialize weights\n",
    "    embedding_layer.build((1, ))\n",
    "    # set weights\n",
    "    embedding_layer.set_weights([E])\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9857d293-f85b-41f3-8001-9e11adc1564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = build_embedding_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bcb08a-893e-40ed-b000-0bb925edb8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass token ids as example\n",
    "embedding_layer(token_ids).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdaec28-9ae7-4f3d-a732-6fc3e43510b6",
   "metadata": {},
   "source": [
    "### Building text encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95575744-c1b5-423c-8303-8f1bbb454edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_encoder_on_glove():\n",
    "    model = models.Sequential()\n",
    "    model.add(vectorizer_layer)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "    # model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48017fd-c7af-4d3f-84fc-50c0a929be94",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = build_text_encoder_on_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf4c5b-eb6d-4c9c-ba29-d4022fc4f2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf22eda4-12e2-4b80-8fb4-25399f1bba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = text_encoder(labels)\n",
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0707f2-bb20-4378-b4be-bf9d05c3f44c",
   "metadata": {},
   "source": [
    "## Multimodal Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d29a6e-b1a3-4df2-9745-ea1baa1a6c7b",
   "metadata": {},
   "source": [
    "### Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dacfd70-74f4-4862-bac7-70444ef35848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(X, y, batch_size=64):\n",
    "    y = np.array([f'A photo of a {labels[target[0]]}' for target in y])\n",
    "    return convert_to_tf_dataset(X, y, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de2cd54-3d5f-4f3d-a9e3-6af8a27806a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = make_dataset(X_train, y_train, batch_size=128)\n",
    "val_ds = make_dataset(X_val, y_val, batch_size=128)\n",
    "test_ds = make_dataset(X_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a577d2e-0e5a-4ba3-981c-2662660413ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_first_batch(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42c1ccd-3f1e-4003-a202-74236ba752a8",
   "metadata": {},
   "source": [
    "### CLIP-like model\n",
    "\n",
    "CLIP (Contrastive Language-Image Pretraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad5b54f-22a0-40d3-8d4e-7aff716b912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Layer to project representation of some modality\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, projection_dim=64, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.dense = layers.Dense(projection_dim)\n",
    "        self.layer_norm = layers.LayerNormalization()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense(inputs)\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e05c8d-4c3a-4e7d-bcf8-718150a16952",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaCLIPModel(Model):\n",
    "    \"\"\"\n",
    "    Simple CLIP-like model to visual-text representation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_encoder, text_encoder, initial_temperature=0.07, projection_dim=64):\n",
    "        super().__init__()\n",
    "        # encoders for modality representation\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        # project to shared multimodal space\n",
    "        self.image_projection = ProjectionLayer(projection_dim)\n",
    "        self.text_projection = ProjectionLayer(projection_dim)\n",
    "        # control logits range\n",
    "        self.logit_scale = self.add_weight(\n",
    "            initializer=tf.constant_initializer(np.log(1.0 / initial_temperature)),\n",
    "            trainable=True,\n",
    "            dtype=tf.float32,\n",
    "            shape=(),\n",
    "            name=\"logit_scale\"\n",
    "        )\n",
    "        # metrics\n",
    "        self.loss_tracker = metrics.Mean(name='loss')\n",
    "        self.accuracy_tracker = metrics.Accuracy(name='accuracy')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # unpack inputs\n",
    "        I, T = inputs\n",
    "        # encode inputs to get features\n",
    "        I_features = self.image_encoder(I, training=training)\n",
    "        T_features = self.text_encoder(T, training=training)\n",
    "        # project to shared multimodal space (multimodal embeddings)\n",
    "        I_projections = self.image_projection(I_features, training=training)\n",
    "        T_projections = self.text_projection(T_features, training=training)\n",
    "        # normalize projections\n",
    "        I_projections = tf.math.l2_normalize(I_projections, axis=1)\n",
    "        T_projections = tf.math.l2_normalize(T_projections, axis=1)\n",
    "        return I_projections, T_projections\n",
    "\n",
    "    def compute_contrastive_loss(self, I_projections, T_projections):\n",
    "        # compute similarity matrix: image-to-text and text-to-image\n",
    "        I_logits = tf.matmul(I_projections, T_projections, transpose_b=True)\n",
    "        I_logits *= tf.exp(self.logit_scale)\n",
    "        T_logits = tf.transpose(I_logits)\n",
    "        # create targets (Y) in one-hot form\n",
    "        batch_size = tf.shape(I_projections)[0]\n",
    "        Y = tf.eye(batch_size)\n",
    "        # compute cross entropy losses\n",
    "        I_loss = tf.keras.losses.categorical_crossentropy(\n",
    "            y_true=Y, \n",
    "            y_pred=I_logits, \n",
    "            from_logits=True\n",
    "        )\n",
    "        T_loss = tf.keras.losses.categorical_crossentropy(\n",
    "            y_true=Y, \n",
    "            y_pred=T_logits, \n",
    "            from_logits=True\n",
    "        )\n",
    "        total_loss = (I_loss + T_loss) / 2\n",
    "        return tf.reduce_mean(total_loss), I_logits\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        # unpack inputs\n",
    "        I, T = data\n",
    "        # record operation graph\n",
    "        with tf.GradientTape() as tape:\n",
    "            # tape.watch(self.logit_scale)\n",
    "            I_projections, T_projections = self.call(\n",
    "                inputs=(I, T), \n",
    "                training=True\n",
    "            )\n",
    "            loss, logits = self.compute_contrastive_loss(\n",
    "                I_projections=I_projections, \n",
    "                T_projections=T_projections\n",
    "            )\n",
    "        # compute gradients and update weights\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        # update metrics\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        y_pred = tf.argmax(logits, axis=1)\n",
    "        y_true = tf.range(tf.shape(logits)[0])\n",
    "        self.accuracy_tracker.update_state(y_true, y_pred)\n",
    "        return {\n",
    "            'loss': self.loss_tracker.result(),\n",
    "            'accuracy': self.accuracy_tracker.result(),\n",
    "            \"temperature\": self.temperature,\n",
    "            \"logit_scale\": self.logit_scale\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # unpack inputs\n",
    "        I, T = data\n",
    "        # compute projections\n",
    "        I_projections, T_projections = self.call(\n",
    "            inputs=(I, T), \n",
    "            training=False\n",
    "        )\n",
    "        # compute loss\n",
    "        loss, logits = self.compute_contrastive_loss(\n",
    "            I_projections=I_projections, \n",
    "            T_projections=T_projections\n",
    "        )\n",
    "        # Update metrics\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        y_pred = tf.argmax(logits, axis=1)\n",
    "        y_true = tf.range(tf.shape(logits)[0])\n",
    "        self.accuracy_tracker.update_state(y_true, y_pred)\n",
    "        return {\n",
    "            \"loss\": self.loss_tracker.result(),\n",
    "            \"accuracy\": self.accuracy_tracker.result()\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def temperature(self):\n",
    "        return 1.0 / tf.exp(self.logit_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e196a2-ea54-4a64-a284-17e4e3647761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "clip_model = VanillaCLIPModel(\n",
    "    image_encoder, \n",
    "    text_encoder, \n",
    "    initial_temperature=0.07\n",
    ")\n",
    "clip_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f20c6d8-8bb2-40a5-941f-a7ed11b940fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    # optimizer=tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72202f8b-d09b-4fed-a4bb-2916102053f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "clip_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476e6db9-bb7b-4e5e-be9d-878a4e0a1688",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a1524-5712-4048-908f-bed49afe2db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ff645e-305f-450b-bf68-9bb202d1e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPInference(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def project_image(self, image):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def project_text(self, text):\n",
    "        pass\n",
    "\n",
    "    def compute_similarity(self, image_projections, text_projections):\n",
    "        return tf.matmul(image_projections, text_projections, transpose_b=True) * tf.exp(self.logit_scale)\n",
    "    \n",
    "    def rank_texts(self, image, texts):\n",
    "        I_projections = self.project_image(image)\n",
    "        T_projections = self.project_text(texts)\n",
    "        I_logits = self.compute_similarity(I_projections, T_projections)\n",
    "        return self.format_rank_output(texts, I_logits)\n",
    "    \n",
    "    def rank_images(self, text, images):\n",
    "        T_projections = self.project_text(text)\n",
    "        I_projections = tf.concat([self.project_image(image) for image in images], axis=0)\n",
    "        T_logits = self.compute_similarity(T_projections, I_projections)\n",
    "        # T_logits = tf.transpose(self.compute_similarity(I_projections, T_projections))\n",
    "        return self.format_rank_output(images, T_logits)\n",
    "\n",
    "    @staticmethod\n",
    "    def format_rank_output(inputs, logits):\n",
    "        indices = tf.argsort(logits, axis=1, direction='DESCENDING')\n",
    "        values = tf.gather(inputs, indices)\n",
    "        probabilities = tf.nn.softmax(logits, axis=1)[0]\n",
    "        probabilities_sorted = tf.gather(probabilities, indices)\n",
    "        return indices.numpy().flatten(), values.numpy()[0], probabilities_sorted.numpy().flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7010f3c-07f7-4bbe-8077-89b2bc7303dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaCLIPInference(CLIPInference):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.image_encoder = model.image_encoder\n",
    "        self.text_encoder = model.text_encoder\n",
    "        self.image_projection = model.image_projection\n",
    "        self.text_projection = model.text_projection\n",
    "        self.logit_scale = model.logit_scale\n",
    "        \n",
    "    def project_image(self, image):\n",
    "        if len(image.shape) == 3:\n",
    "            image = tf.expand_dims(image, 0)\n",
    "        I_features = self.image_encoder(image, training=False)\n",
    "        I_projections = self.image_projection(I_features, training=False)\n",
    "        return tf.math.l2_normalize(I_projections, axis=1)\n",
    "    \n",
    "    def project_text(self, text):\n",
    "        if isinstance(text, str):\n",
    "            text = tf.constant([text])\n",
    "        T_features = self.text_encoder(text, training=False)\n",
    "        T_projections = self.text_projection(T_features, training=False)\n",
    "        return tf.math.l2_normalize(T_projections, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f176d42-2980-4214-a013-daf5207acd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = VanillaCLIPInference(clip_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a18af4-6291-4ee6-a06f-9f9fb87b95ec",
   "metadata": {},
   "source": [
    "**Projecting to multimodal space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4137403e-cd05-4cfc-a7fe-8a1dc743722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = X_test[10]\n",
    "test_text = \"image of airplane\"\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(test_image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa063ff5-5f46-476d-8f7c-688d70281d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "I_projections = inference.project_image(test_image)\n",
    "I_projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121290e2-0fa7-46a0-b2ff-ccc356880271",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_projections = inference.project_text(test_text)\n",
    "T_projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd251c09-f938-4178-ba98-72b30260a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference.compute_similarity(I_projections, T_projections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b54bf1-ce7b-43ae-8c0e-f483b3c0da46",
   "metadata": {},
   "source": [
    "**Ranking texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570fe0c0-4d29-40eb-8a2f-58d453c7768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = X_test[15]\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(test_image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "indices, values, probs = inference.rank_texts(test_image, labels)\n",
    "indices, values, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2c3ebf-50a7-4efc-822c-0c7ab0e6ab14",
   "metadata": {},
   "source": [
    "**Ranking images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656efa5e-2879-4d3c-b1e1-185785696802",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"Image of an airplane\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a9fd7c-0450-4c92-91fd-8a7ce16d682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, values, probs = inference.rank_images(test_text, X_test[:100])\n",
    "indices, values.shape, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1892bcc-c008-4937-84b0-31043e558fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[14, 4])\n",
    "for index, image in enumerate(values[:10]):\n",
    "    plt.subplot(1, NUM_DISPLAY_IMAGES, index+1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834ae75a-edc9-4b04-9a3e-5d5ebb1310e5",
   "metadata": {},
   "source": [
    "## Pretrained `CLIP` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb53d8a-487d-41b9-ae82-7913b82c3c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, TFCLIPModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493deec4-fa8e-4fb6-ae91-ab15caf9c782",
   "metadata": {},
   "source": [
    "### Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faddc35-1ffd-4b46-83cb-aba99975975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = \"openai/clip-vit-base-patch32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af0498a-0992-484e-918c-d083fc5e055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(CHECKPOINT)\n",
    "model = TFCLIPModel.from_pretrained(CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1e20eb-719a-4cc8-a944-b4b8a684d335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340d063b-c334-4263-b82f-bda4020ca155",
   "metadata": {},
   "source": [
    "### Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad01cf-18c1-4674-bfb7-7f57edd2f84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_INDEX = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a5e45e-a07f-4663-9b77-2de8993990e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single sample\n",
    "image = X_test[IMAGE_INDEX]\n",
    "target = y_test[IMAGE_INDEX]\n",
    "label = labels[target]\n",
    "text = 'a photo of a cat'\n",
    "\n",
    "image.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb91ae-a68f-4943-99bf-3955bc347d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show image\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0731f3ab-138b-402f-8892-4e45368e2fb9",
   "metadata": {},
   "source": [
    "**Image processor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca3f73-e03c-4ca0-ad8a-a58ecfc87b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = processor.image_processor(image)\n",
    "image_input.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe3b100-6ba1-499a-ba20-4fad0b53cacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input.pixel_values[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462c8a4b-6668-4dac-9883-2becd48c7961",
   "metadata": {},
   "source": [
    "**Text tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05f0686-c047-4617-bbc2-8af1cc497c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = processor.tokenizer(text)\n",
    "text_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3404f3-083e-4db2-84da-8188c51632ea",
   "metadata": {},
   "source": [
    "**Processor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20340ecd-1361-4286-8095-2f8359bf84ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=text, \n",
    "    images=image, \n",
    "    return_tensors=\"tf\", \n",
    "    padding=True\n",
    ")\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8252472-500a-4772-9c05-988df7776bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "I_pixels = inputs.pixel_values\n",
    "T_ids = inputs.input_ids\n",
    "T_mask = inputs.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0ab353-ab8d-4a55-ad0d-e32047c8901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "I_pixels.shape, T_ids, T_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47a6ba0-6808-46d9-ae02-8fdcbaa14eab",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bced60-df70-42eb-8aa9-a595a406f42e",
   "metadata": {},
   "source": [
    "**Model outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42bdbe5-413b-494d-b526-019e1ded4102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input single image and list of texts (class labels for simplicity)\n",
    "images = image\n",
    "texts = labels.tolist()\n",
    "\n",
    "images.shape, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217badd-3db6-446f-adb0-770024a8a703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process model inputs\n",
    "inputs = processor(\n",
    "    text=texts, \n",
    "    images=images, \n",
    "    return_tensors=\"tf\", \n",
    "    padding=True\n",
    ")\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35c07bc-d4d5-49f1-9317-d807c98c47d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "outputs = model(**inputs, training=False)\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27708593-4cab-4018-8bd6-4bf193790d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized projections\n",
    "I_projections = outputs.image_embeds\n",
    "T_projections = outputs.text_embeds\n",
    "\n",
    "I_projections.shape, T_projections.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a089986-1180-40a5-b6c4-deb33fc93ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity matrix\n",
    "I_logits = outputs.logits_per_image\n",
    "T_logits = outputs.logits_per_text\n",
    "\n",
    "I_logits.shape, T_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb2f02a-4904-41e7-8054-46c2b92ee2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.linalg.norm(I_projections.numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed5fff0-a560-4c35-9ab1-07d3da22dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unnormalized projections\n",
    "I_projections_unnorm = model.get_image_features(\n",
    "    pixel_values=inputs['pixel_values']\n",
    ")\n",
    "T_projections_unnorm = model.get_text_features(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask']\n",
    ")\n",
    "\n",
    "I_projections_unnorm.shape, T_projections_unnorm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43b726b-1782-4a08-b283-ad0648d1f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank texts by image\n",
    "CLIPInference.format_rank_output(texts, I_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba5bd35-ba1a-40e7-a2af-062e847b29b7",
   "metadata": {},
   "source": [
    "**Inference class implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390863b3-283a-4798-98d3-703f1288ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerCLIPInference(CLIPInference):\n",
    "    \n",
    "    def __init__(self, model, processor):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.logit_scale = self._get_logit_scale(model)\n",
    "\n",
    "    def _get_logit_scale(self, model):\n",
    "        for var in model.trainable_variables:\n",
    "            if 'logit_scale' in var.name.lower():\n",
    "                return var\n",
    "        return tf.Variable(0.07)\n",
    "        \n",
    "    def project_image(self, image):\n",
    "        if len(image.shape) == 3:\n",
    "            image = tf.expand_dims(image, 0)\n",
    "        inputs = processor(\n",
    "            images=image, \n",
    "            return_tensors=\"tf\", \n",
    "            padding=True\n",
    "        )\n",
    "        I_projections_unorm = model.get_image_features(**inputs)\n",
    "        return tf.math.l2_normalize(I_projections_unorm, axis=1)\n",
    "    \n",
    "    def project_text(self, text):\n",
    "        if isinstance(text, np.ndarray):\n",
    "            text = text.tolist()\n",
    "        inputs = processor(\n",
    "            text=text, \n",
    "            return_tensors=\"tf\", \n",
    "            padding=True\n",
    "        )\n",
    "        I_projections_unorm = model.get_text_features(**inputs)\n",
    "        return tf.math.l2_normalize(I_projections_unorm, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cd0a96-dfb6-4b58-9236-5615eca9c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = TransformerCLIPInference(model, processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e1c457-8bf2-4eb7-8682-2b92c26e9236",
   "metadata": {},
   "source": [
    "**Ranking texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c453df06-11f4-4dd9-b99b-6f97113d058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = X_test[15]\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(test_image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "indices, values, probs = inference.rank_texts(test_image, labels)\n",
    "indices, values, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573f2151-7278-44b4-b1ae-77229acf4895",
   "metadata": {},
   "source": [
    "**Ranking images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c1ed20-add6-416e-8c71-b6b6cc46094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"Image of a car\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c34505-4c98-468e-bc48-ced3335261b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, values, probs = inference.rank_images(test_text, X_test[:100])\n",
    "indices, values.shape, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d76a9-92ff-49bd-a786-9beff4db148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[14, 4])\n",
    "for index, image in enumerate(values[:10]):\n",
    "    plt.subplot(1, NUM_DISPLAY_IMAGES, index+1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39674fd2-7384-4180-8981-085e276a1dc2",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236ce2e0-820a-4228-a993-089133552a1a",
   "metadata": {},
   "source": [
    "- [Radford, A., et al. Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)\n",
    "- [Transformers documentation: CLIP](https://huggingface.co/docs/transformers/model_doc/clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760c8528-a13b-40bf-944e-03e64e1466cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
