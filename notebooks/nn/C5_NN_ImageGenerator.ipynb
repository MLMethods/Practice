{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61eb1697-6494-4e8c-9607-eda299f0cca1",
   "metadata": {},
   "source": [
    "# Image Generator\n",
    "---\n",
    "\n",
    "S.Yu. Papulin (papulin.study@yandex.ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f7d11d-e4b1-493b-8a6f-1a0433c9d849",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "- [GAN Model](#GAN-Model)\n",
    "    - [Preparing dataset](#Preparing-dataset)\n",
    "    - [Generator](#Generator)\n",
    "    - [Discriminator](#Discriminator)\n",
    "    - [Image generator based on GAN](#Image-generator-based-on-GAN)\n",
    "    - [Conditional GAN with classifier](#Conditional-GAN-with-classifier)\n",
    "    - [Conditional GAN](#Conditional-GAN)\n",
    "    - [Generating image based on text prompt](#Generating-image-based-on-text-prompt)\n",
    "- [Stable Diffusion Model (Pretrained)](#Stable-Diffusion-Model-(Pretrained))\n",
    "- [Sources](#Sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ddabae-3ceb-4d10-8de4-22e72541572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127084be-f954-49c1-a84c-20da2d1c4cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import (\n",
    "    layers, \n",
    "    models, \n",
    "    Model, \n",
    "    utils, \n",
    "    losses, \n",
    "    optimizers, \n",
    "    metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe9cde-5780-42ce-9a3e-54a96b7fd44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3e38ab-7f01-4707-990c-e491f614c2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902bd940-19d5-406a-a095-f5779742ea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2d0665-2468-44f4-aa7d-b0112e9bc015",
   "metadata": {},
   "source": [
    "## GAN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce96bf45-8938-4ca8-9804-aa43b6104017",
   "metadata": {},
   "source": [
    "Generative adversarial network - GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeff33a-c110-4531-bdf7-ed807c76cad0",
   "metadata": {},
   "source": [
    "### Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5487a067-00ae-4722-b296-9eb602f02106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_8x8():\n",
    "    from sklearn import datasets\n",
    "    digits = datasets.load_digits()\n",
    "    X = digits.images\n",
    "    y = digits.target\n",
    "    X = X.astype('float32') / 16.0\n",
    "    X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2, \n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    return (X_trainval, y_trainval), (X_test, y_test)\n",
    "\n",
    "\n",
    "def load_mnist_28x28():\n",
    "    from tensorflow.keras.datasets import mnist\n",
    "    (X_trainval, y_trainval), (X_test, y_test) = mnist.load_data()\n",
    "    return (X_trainval / 255.0, y_trainval), (X_test / 255.0, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244b0882-adbe-4103-8337-b34a20b27b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_trainval, y_trainval), (X_test, y_test) = load_mnist_8x8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8aa1ab-3ce3-4b1c-8487-e6300bb8007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainval.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4da130e-eae2-480d-a0f6-702674d7971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainval.min(), X_trainval.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b919519a-5bea-4b4e-b555-0c120ed7a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets, counts = np.unique(y_trainval, return_counts=True)\n",
    "targets, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cffd52e-6bd9-44e6-b6f6-d8b366342f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(targets)\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106814fc-7bf4-4a4f-a55e-4bee7f8f52e4",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b0bf5d-97de-49f1-b41b-3159c9ce5e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality of latent noise vector\n",
    "NOISE_DIM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe786cf3-26ae-48b9-96f4-5dce6fe7b04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_generator_model(noise_dim=NOISE_DIM):\n",
    "    model = models.Sequential(name='generator')\n",
    "    model.add(layers.Input(shape=(noise_dim,)))\n",
    "    model.add(layers.Dense(16, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(8 * 8 * 1, activation='tanh'))\n",
    "    model.add(layers.Reshape((8, 8, 1)))\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_advanced_generator_model(noise_dim=NOISE_DIM):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(noise_dim,)))\n",
    "    # input embedding layer\n",
    "    model.add(layers.Dense(4*4*64, activation='leaky_relu'))\n",
    "    # reshape (like 4x4 per 64 filters)\n",
    "    model.add(layers.Reshape(target_shape=(4, 4, 64)))\n",
    "    # upscale\n",
    "    # 4x4\n",
    "    # model.add(layers.Conv2DTranspose(\n",
    "    #     filters=32, \n",
    "    #     kernel_size=(2, 2), \n",
    "    #     strides=(1, 1), \n",
    "    #     padding='same'\n",
    "    # ))\n",
    "    # model.add(layers.LeakyReLU(0.02))\n",
    "    # model.add(layers.BatchNormalization())\n",
    "    # 8x8\n",
    "    model.add(layers.Conv2DTranspose(\n",
    "        filters=16, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(2, 2), \n",
    "        padding='same'\n",
    "    ))\n",
    "    model.add(layers.LeakyReLU(0.02))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    # 8x8\n",
    "    model.add(layers.Conv2DTranspose(\n",
    "        filters=1, \n",
    "        kernel_size=(5, 5), \n",
    "        strides=(1, 1), \n",
    "        padding='same',\n",
    "        activation='tanh'\n",
    "    ))\n",
    "    return model\n",
    "\n",
    "# Upscaling block\n",
    "#     model.add(layers.UpSampling2D((2, 2)))\n",
    "#     model.add(layers.Conv2D(filters=16, kernel_size=3, padding='same'))\n",
    "#     model.add(layers.LeakyReLU())\n",
    "#     model.add(layers.BatchNormalization())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c3e8df-84b3-49bd-a38c-7e63576ada8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model = build_advanced_generator_model()\n",
    "generator_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea58601d-2c92-4136-881e-d677f568b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate normal noise\n",
    "sample_noise = tf.random.normal([1, 10])\n",
    "# Generate image\n",
    "output = generator_model(sample_noise)\n",
    "output.shape, tf.reduce_min(output), tf.reduce_max(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70caff8b-83a3-42b0-9886-39b5fbcb5f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show image\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(output[0])\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f27e5b-aeb8-49e6-8c1a-dccc472d1033",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138bdc19-a0e7-47bd-86bd-22c17d51b9a8",
   "metadata": {},
   "source": [
    "**Building dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f735c2-f167-4f8c-b8ac-30d7befa5f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = X_trainval.shape[1]\n",
    "image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f6a8c7-a33f-4dcd-bbd9-2c24a3bafa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(X_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dc1a74-ca41-42ba-9c99-652883b1019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True images\n",
    "X_disc_true = X_trainval.reshape(-1, image_size, image_size, 1)\n",
    "y_disc_true = np.ones(n_samples)\n",
    "\n",
    "X_disc_true.shape, y_disc_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce61b4a-25d0-44da-8711-084b0276709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake image\n",
    "sample_noise = tf.random.normal([n_samples, NOISE_DIM])\n",
    "X_disc_fake = generator_model(sample_noise)\n",
    "y_disc_fake = np.zeros(n_samples)\n",
    "\n",
    "X_disc_fake.shape, y_disc_fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b62a30-1b00-4d86-96f5-3aa7c2f5b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset by combining true and fake images\n",
    "X_disc = np.r_[X_disc_true, X_disc_fake]\n",
    "y_disc = np.r_[y_disc_true, y_disc_fake]\n",
    "\n",
    "X_disc.shape, y_disc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3106d2e-e10a-41bc-a594-31bb1b6c0c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_disc_trainval, X_disc_test, y_disc_trainval, y_disc_test = train_test_split(\n",
    "    X_disc, y_disc, \n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "X_disc_trainval.shape, X_disc_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb58a69f-f653-4d68-b05b-f26448410989",
   "metadata": {},
   "source": [
    "**Building discriminator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062728ca-37b8-4574-8e3b-9e2bf5c60d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_discriminator_model():\n",
    "    model = models.Sequential(name='discriminator')\n",
    "    model.add(layers.Input(shape=(8, 8, 1)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_advanced_discriminator_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(8, 8, 1)))\n",
    "    model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc094622-1f2d-48fa-893c-39f71879737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build discrimator model\n",
    "discriminator_model = build_simple_discriminator_model()\n",
    "discriminator_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885a2d1e-ff00-4e93-a926-69389cde189a",
   "metadata": {},
   "source": [
    "**Fitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de20784-bc64-4bc3-ab3d-ba4cf8d51ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_model.compile(\n",
    "    optimizer=optimizers.Adam(1e-3),\n",
    "    loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[metrics.BinaryAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc572ee-a10d-4d75-b8ba-16f8b11e7364",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_model.fit(\n",
    "    x=X_disc_trainval,\n",
    "    y=y_disc_trainval,\n",
    "    validation_split=0.1,\n",
    "    batch_size=64,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36a4724-82a1-477f-9d1b-1e99e39e5f2f",
   "metadata": {},
   "source": [
    "**Evaluating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e575eea-2fa1-42ad-b3b0-2100a437f3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output metrics on test set\n",
    "discriminator_model.evaluate(X_disc_test, y_disc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890759bf-98db-48e9-97e1-24a795dcf461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute logits, pobabilities and predictions\n",
    "logits = discriminator_model(X_disc_test[:10])\n",
    "pobabilities = tf.nn.sigmoid(logits)\n",
    "predictions = tf.round(pobabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6d932f-0cb8-4662-bfb8-2ff373a493a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "tf.reshape(predictions, shape=(-1)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b37cad5-112f-4277-8f08-8e79778b1a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True values\n",
    "y_disc_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2e63e1-8ce3-4afd-a0a4-2dd2dcfe0cfc",
   "metadata": {},
   "source": [
    "### Image generator based on GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bbd9dc-6b69-4d73-956b-8bf0099f5c9e",
   "metadata": {},
   "source": [
    "**Prepatring dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a224e5ab-8306-48e1-8d9b-010fe8fba56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tf_dataset(X, y, batch_size=64):\n",
    "    return (\n",
    "        tf.data.Dataset.from_tensor_slices((X, y))\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed490737-532c-4ae1-95a0-6c02beb83225",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainval.shape, y_trainval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ee5470-a0f5-4f38-8ffa-4595295ae40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, \n",
    "    test_size=0.1, \n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa331544-3111-47b5-9661-a424d86bd844",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = convert_to_tf_dataset(X_train, y_train, batch_size=64)\n",
    "val_ds = convert_to_tf_dataset(X_val, y_val, batch_size=64)\n",
    "test_ds = convert_to_tf_dataset(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c3a938-56d5-439c-8553-dcb174817c5a",
   "metadata": {},
   "source": [
    "#### Generating image similar to training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668f7aab-101a-4386-8284-58a83703b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaGANModel(Model):\n",
    "\n",
    "    def __init__(self, generator_model, discriminator_model, **kwargs):\n",
    "        super().__init__(**kwargs) \n",
    "        # models\n",
    "        self.generator = generator_model\n",
    "        self.discriminator = discriminator_model\n",
    "        # optimizers\n",
    "        self.generator_optimizer = optimizers.Adam(1e-4)\n",
    "        self.discriminator_optimizer = optimizers.Adam(1e-4)\n",
    "        # metrics\n",
    "        self.train_generator_loss = metrics.Mean(name='train_generator_loss')\n",
    "        self.train_discriminator_loss = metrics.Mean(name='train_discriminator_loss')\n",
    "        self.test_generator_loss = metrics.Mean(name='test_generator_loss')\n",
    "        self.test_discriminator_loss = metrics.Mean(name='test_discriminator_loss')\n",
    "        self.train_cheater_accuracy = metrics.BinaryAccuracy(name='train_accuracy')\n",
    "        self.test_cheater_accuracy = metrics.BinaryAccuracy(name='test_accuracy')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        batch_size = inputs\n",
    "        # generate noise\n",
    "        noise = tf.random.normal([batch_size, NOISE_DIM])\n",
    "        # generate fake images\n",
    "        X_fake = self.generator(noise)\n",
    "        return X_fake\n",
    "\n",
    "    def compute_generator_loss(self, X_fake):\n",
    "        # classify whether a fake image is real\n",
    "        y_fake_logits = self.discriminator(X_fake)\n",
    "        # penalty for generating unrealistic images\n",
    "        loss = losses.binary_crossentropy(\n",
    "            y_true=tf.ones_like(y_fake_logits), \n",
    "            y_pred=y_fake_logits, \n",
    "            from_logits=True\n",
    "        )\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    def compute_discriminator_loss(self, X_true, X_fake):\n",
    "        \"\"\"\n",
    "        1 - real image\n",
    "        0 - fake image\n",
    "        \"\"\"\n",
    "        # classify whether a real image is real\n",
    "        y_true_logits = self.discriminator(X_true)\n",
    "        # classify whether a fake image is real\n",
    "        y_fake_logits = self.discriminator(X_fake)\n",
    "        # penalty for misclassification of real images\n",
    "        loss_true = losses.binary_crossentropy(\n",
    "            y_true=tf.ones_like(y_true_logits), \n",
    "            y_pred=y_true_logits, \n",
    "            from_logits=True\n",
    "        )\n",
    "        # penalty for misclassification of fake images\n",
    "        loss_fake = losses.binary_crossentropy(\n",
    "            y_true=tf.zeros_like(y_fake_logits), \n",
    "            y_pred=y_fake_logits,\n",
    "            from_logits=True\n",
    "        )\n",
    "        total_loss = (loss_true + loss_fake) / 2.0\n",
    "        return tf.reduce_mean(total_loss), y_fake_logits\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        \n",
    "        X_true, _ = data\n",
    "        \n",
    "        with (\n",
    "            tf.GradientTape() as tape_gen, \n",
    "            tf.GradientTape() as tape_disc\n",
    "        ):\n",
    "            # number of images to generate\n",
    "            batch_size = tf.shape(X_true)[0]\n",
    "            # generate fake images\n",
    "            X_fake = self.call(batch_size)\n",
    "            # compute losses\n",
    "            discriminator_loss, y_fake_logits = self.compute_discriminator_loss(X_true, X_fake)\n",
    "            generator_loss = self.compute_generator_loss(X_fake)\n",
    "\n",
    "        # compute gradients and update weights\n",
    "        gradients_discriminator = tape_disc.gradient(\n",
    "            target=discriminator_loss, \n",
    "            sources=self.discriminator.trainable_variables\n",
    "        )\n",
    "        self.discriminator_optimizer.apply_gradients(\n",
    "            zip(gradients_discriminator, self.discriminator.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        gradients_generator = tape_gen.gradient(\n",
    "            target=generator_loss, \n",
    "            sources=self.generator.trainable_variables\n",
    "        )\n",
    "        self.generator_optimizer.apply_gradients(\n",
    "            zip(gradients_generator, self.generator.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # update metrics\n",
    "        self.train_discriminator_loss.update_state(discriminator_loss)\n",
    "        self.train_generator_loss.update_state(generator_loss)\n",
    "        self.train_cheater_accuracy.update_state(\n",
    "            y_true=tf.ones_like(y_fake_logits),\n",
    "            y_pred=tf.nn.sigmoid(y_fake_logits)\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'generator_loss': self.train_generator_loss.result(), \n",
    "            'discriminator_loss': self.train_discriminator_loss.result(),\n",
    "            'cheater_accuracy': self.train_cheater_accuracy.result()\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        X_true, _ = data\n",
    "        batch_size = tf.shape(X_true)[0]\n",
    "        # generate fake images\n",
    "        X_fake = self.call(batch_size)\n",
    "        # compute losses\n",
    "        generator_loss = self.compute_generator_loss(X_fake)\n",
    "        discriminator_loss, y_fake_logits = self.compute_discriminator_loss(X_true, X_fake)\n",
    "        # update metrics\n",
    "        self.test_discriminator_loss.update_state(discriminator_loss)\n",
    "        self.test_generator_loss.update_state(generator_loss)\n",
    "        self.test_cheater_accuracy.update_state(\n",
    "            y_true=tf.ones_like(y_fake_logits),\n",
    "            y_pred=tf.nn.sigmoid(y_fake_logits)\n",
    "        )\n",
    "        return {\n",
    "            'generator_loss': self.test_generator_loss.result(), \n",
    "            'discriminator_loss': self.test_discriminator_loss.result(),\n",
    "            'cheater_accuracy': self.test_cheater_accuracy.result()\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32188ee6-0aa0-4212-b88c-545712d5c049",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "gan_model = VanillaGANModel(\n",
    "    generator_model=build_simple_generator_model(),\n",
    "    discriminator_model=build_simple_discriminator_model()\n",
    ")\n",
    "gan_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59226ae-3b78-4370-8cbd-ce9153153ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 300\n",
    "# gan_model = VanillaGANModel(\n",
    "#     generator_model=build_advanced_generator_model(),\n",
    "#     discriminator_model=build_advanced_discriminator_model()\n",
    "# )\n",
    "# gan_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afe767f-44e5-417f-8a99-33d1d286af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24b1120-6e03-4864-89b3-8b5fe2bd20ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NUM_EPOCHS = 500\n",
    "\n",
    "train_history = gan_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    verbose=1 # change to 0 for the simple model\n",
    ")\n",
    "train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068fec96-5b2f-4a77-a359-3d12e9c35cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_image = gan_model(tf.constant(30))\n",
    "fake_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7d0ec0-8ab3-4181-9ef6-8f43781ad29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(I):\n",
    "    NUM_PER_ROW = 10\n",
    "    num_images = I.shape[0]\n",
    "    num_rows = -(-num_images // NUM_PER_ROW)\n",
    "    plt.figure(figsize=[14, 1.5 * num_rows])\n",
    "    for index, image in enumerate(I):\n",
    "        plt.subplot(num_rows, NUM_PER_ROW, index+1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1d464d-ce57-4a17-94b0-6de813c50829",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(fake_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e01bff-4f1b-4439-9c65-72ce92198220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.discriminator(fake_image).numpy() > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20582700-7332-4091-b778-e8880f878446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_smooth_plot(x, y, color, label, step = 10):\n",
    "    # polyline\n",
    "    coefs = np.polyfit(x, y, 5)\n",
    "    poly_func = np.poly1d(coefs)\n",
    "    x_trend = np.linspace(min(x), max(x), 100)\n",
    "    y_trend = poly_func(x_trend)\n",
    "    # mean by step\n",
    "    y_lim = y[:len(y) // step * step]\n",
    "    y_means = np.array(y_lim).reshape(-1, step).mean(axis=1)\n",
    "    x_means = x[:-step:step] + (x[step] - x[0]) / 2.0\n",
    "    plt.plot(x, y, color=color, alpha=0.3, linestyle='-')\n",
    "    plt.plot(x_means, y_means, color=color, linestyle='-', alpha=0.5)\n",
    "    plt.plot(x_trend, y_trend, color=color, linestyle='-', label=label)\n",
    "\n",
    "\n",
    "def display_loss_plots(train_history):\n",
    "    plt.figure(figsize=[14, 8])\n",
    "    epochs = np.arange(1, len(train_history.history['generator_loss'])+1)\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.title('Training')\n",
    "    display_smooth_plot(\n",
    "        x=epochs[1:],\n",
    "        y=train_history.history['generator_loss'][1:],\n",
    "        color='g',\n",
    "        label='generator'\n",
    "    )\n",
    "    display_smooth_plot(\n",
    "        x=epochs[1:],\n",
    "        y=train_history.history['discriminator_loss'][1:],\n",
    "        color='orange',\n",
    "        label='discriminator'\n",
    "    )\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.title('Validation')\n",
    "    display_smooth_plot(\n",
    "        x=epochs[1:],\n",
    "        y=train_history.history['val_generator_loss'][1:],\n",
    "        color='g',\n",
    "        label='generator'\n",
    "    )\n",
    "    display_smooth_plot(\n",
    "        x=epochs[1:],\n",
    "        y=train_history.history['val_discriminator_loss'][1:],\n",
    "        color='orange',\n",
    "        label='discriminator'\n",
    "    )\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.title('Training Accuracy')\n",
    "    display_smooth_plot(\n",
    "        x=epochs[1:],\n",
    "        y=train_history.history['cheater_accuracy'][1:],\n",
    "        color='g',\n",
    "        label='generator'\n",
    "    )\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.title('Validation Accuracy')\n",
    "    display_smooth_plot(\n",
    "        x=epochs[1:],\n",
    "        y=train_history.history['val_cheater_accuracy'][1:],\n",
    "        color='g',\n",
    "        label='generator'\n",
    "    )\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef26907-165e-4e42-a2f5-6165a44d515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_loss_plots(train_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4113feb0-9798-4946-b85a-f60c7b8b64a0",
   "metadata": {},
   "source": [
    "### Conditional GAN with classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59808b9e-f977-47ab-bb2a-3ce9ede1a2e0",
   "metadata": {},
   "source": [
    "#### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264d8e90-50a5-49d7-90a2-222aaa4e14bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    model = models.Sequential(name='classifier')\n",
    "    model.add(layers.Input(shape=(8, 8, 1)))\n",
    "    model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bd68f3-5ab2-44c8-8d66-01a4d35d3031",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "classifier_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c8e7d3-97a0-493c-9b9a-dacd1030773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-3), \n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[metrics.SparseCategoricalAccuracy(),]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913bc586-28ef-4ae7-9ebf-bae22aac1d06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier_model.fit(\n",
    "    train_ds,\n",
    "    epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8873693-56f5-4e50-a687-ea3d2cc55e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fadd35-40fa-47c2-8626-c72729cf72b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off training\n",
    "classifier_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a4117d-c606-415e-9d68-3459ddd4f03a",
   "metadata": {},
   "source": [
    "#### GAN generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d080f5-cb75-4aef-bb9e-e7a0189b96b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_generator_model(noise_dim=NOISE_DIM):\n",
    "    # inputs\n",
    "    inputs_noise = layers.Input(shape=(noise_dim,))\n",
    "    inputs_target = layers.Input(shape=(1,))\n",
    "\n",
    "    # refactor inputs\n",
    "    x_noise = layers.Dense(4*4*NUM_CLASSES, use_bias=False)(inputs_noise)\n",
    "    x_target = layers.CategoryEncoding(\n",
    "        num_tokens=NUM_CLASSES, \n",
    "        output_mode='one_hot'\n",
    "    )(inputs_target)\n",
    "\n",
    "    # combine flows from different inputs\n",
    "    x = layers.Concatenate()([x_noise, x_target])\n",
    "\n",
    "    # generator\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(8 * 8 * 1, activation='tanh')(x)\n",
    "\n",
    "    # output\n",
    "    output = layers.Reshape((8, 8, 1))(x)\n",
    "\n",
    "    return models.Model(\n",
    "        inputs=[inputs_noise, inputs_target], \n",
    "        outputs=output, \n",
    "        name='generator'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b87950d-773f-457b-9b96-556d587c98e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_advanced_generator_model(noise_dim=NOISE_DIM):\n",
    "\n",
    "    # inputs\n",
    "    inputs_noise = layers.Input(shape=(noise_dim,))\n",
    "    inputs_target = layers.Input(shape=(1,))\n",
    "    \n",
    "    # refactor inputs\n",
    "    x_noise = layers.Dense(4*4*NUM_CLASSES, use_bias=False)(inputs_noise)\n",
    "    x_target = layers.CategoryEncoding(num_tokens=NUM_CLASSES, output_mode=\"one_hot\")(inputs_target)\n",
    "    # x_target = layers.Dense(10, activation='relu')(x_target)\n",
    "    x_target = layers.Dense(512, activation='relu')(x_target)\n",
    "    x_target = layers.Dense(4*4*NUM_CLASSES, activation='relu')(x_target)\n",
    "    x_target = layers.LayerNormalization()(x_target)\n",
    "    \n",
    "    # generator\n",
    "    x = layers.Add()([x_noise, x_target])\n",
    "    x = layers.Reshape(target_shape=(4, 4, NUM_CLASSES))(x)\n",
    "    # upscale\n",
    "    # 4x4\n",
    "    # x = layers.Conv2DTranspose(\n",
    "    #     filters=64, \n",
    "    #     kernel_size=(3, 3), \n",
    "    #     strides=(1, 1), \n",
    "    #     padding='same'\n",
    "    # )(x)\n",
    "    # x = layers.BatchNormalization()(x)         \n",
    "    # x = layers.LeakyReLU()(x)\n",
    "    # 8x8\n",
    "    x = layers.Conv2DTranspose(\n",
    "        filters=16, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(2, 2), \n",
    "        padding='same'\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    \n",
    "    # output image\n",
    "    output = layers.Conv2DTranspose(\n",
    "        filters=1, \n",
    "        kernel_size=(5, 5), \n",
    "        strides=(1, 1), \n",
    "        padding='same',\n",
    "        activation='tanh'\n",
    "    )(x)\n",
    "    return Model(\n",
    "        inputs=[inputs_noise, inputs_target], \n",
    "        outputs=output, \n",
    "        name='generator'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c61d479-31ef-4e2a-84e5-424e7364eb30",
   "metadata": {},
   "source": [
    "#### Building conditional GAN with classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c8b557-b996-4aac-a236-2a572a4e95ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalClassifierGANModel(Model):\n",
    "\n",
    "    def __init__(self, generator_model, discriminator_model, classifier_model, **kwargs):\n",
    "        super().__init__(**kwargs) \n",
    "        # models\n",
    "        self.generator = generator_model\n",
    "        self.discriminator = discriminator_model\n",
    "        self.classifier = classifier_model\n",
    "        # optimizers\n",
    "        self.generator_optimizer = optimizers.Adam(1e-4)\n",
    "        self.discriminator_optimizer = optimizers.Adam(1e-4)\n",
    "        # metrics\n",
    "        self.train_generator_loss = metrics.Mean(name='train_generator_loss')\n",
    "        self.train_discriminator_loss = metrics.Mean(name='train_discriminator_loss')\n",
    "        self.test_generator_loss = metrics.Mean(name='test_generator_loss')\n",
    "        self.test_discriminator_loss = metrics.Mean(name='test_discriminator_loss')\n",
    "        self.train_cheater_accuracy = metrics.BinaryAccuracy(name='train_accuracy')\n",
    "        self.test_cheater_accuracy = metrics.BinaryAccuracy(name='test_accuracy')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        y_fake = inputs\n",
    "        noise = tf.random.normal([tf.shape(y_fake)[0], NOISE_DIM])\n",
    "        X_fake = self.generator([noise, y_fake])\n",
    "        return X_fake\n",
    "\n",
    "    def compute_discriminator_loss(self, X_true, X_fake):\n",
    "        y_true_logits = self.discriminator(X_true)\n",
    "        y_fake_logits = self.discriminator(X_fake)\n",
    "        loss_true = losses.binary_crossentropy(\n",
    "            y_true=tf.ones_like(y_true_logits), \n",
    "            y_pred=y_true_logits, \n",
    "            from_logits=True\n",
    "        )\n",
    "        loss_fake = losses.binary_crossentropy(\n",
    "            y_true=tf.zeros_like(y_fake_logits), \n",
    "            y_pred=y_fake_logits,\n",
    "            from_logits=True\n",
    "        )\n",
    "        total_loss = (loss_true + loss_fake) / 2.0\n",
    "        return tf.reduce_mean(total_loss), y_fake_logits\n",
    "\n",
    "    def compute_generator_loss(self, X_fake, y_fake):\n",
    "        y_fake_logits = self.discriminator(X_fake)\n",
    "        y_fake_target_probs = self.classifier(X_fake)\n",
    "        generator_loss = losses.binary_crossentropy(\n",
    "            y_true=tf.ones_like(y_fake_logits), \n",
    "            y_pred=y_fake_logits, \n",
    "            from_logits=True\n",
    "        )\n",
    "        classifier_loss = losses.sparse_categorical_crossentropy(\n",
    "            y_true=y_fake,\n",
    "            y_pred=y_fake_target_probs\n",
    "        )\n",
    "        total_loss = generator_loss + 2.0 * classifier_loss\n",
    "        return tf.reduce_mean(total_loss)\n",
    "\n",
    "    def compute_generator_loss_without_classifier(self, X_fake, y_fake):\n",
    "        y_fake_logits = self.discriminator(X_fake)\n",
    "        y_fake_target_probs = self.classifier(X_fake)\n",
    "        generator_loss = losses.binary_crossentropy(\n",
    "            y_true=tf.ones_like(y_fake_logits), \n",
    "            y_pred=y_fake_logits, \n",
    "            from_logits=True\n",
    "        )\n",
    "        return tf.reduce_mean(generator_loss)\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "\n",
    "        # unpack input data\n",
    "        X_true, _ = data\n",
    "        \n",
    "        with (\n",
    "            tf.GradientTape() as tape_gen, \n",
    "            tf.GradientTape() as tape_disc\n",
    "        ):\n",
    "            # number of images to generate\n",
    "            batch_size = tf.shape(X_true)[0]\n",
    "            # generate y for fake images\n",
    "            y_fake = tf.random.uniform([batch_size], minval=0, maxval=10, dtype=tf.int32)\n",
    "            # generate fake images\n",
    "            X_fake = self.call(y_fake, training=True)\n",
    "            # compute losses\n",
    "            discriminator_loss, y_fake_logits = self.compute_discriminator_loss(X_true, X_fake)\n",
    "            generator_loss = self.compute_generator_loss(X_fake, y_fake)\n",
    "            \n",
    "        # compute gradients and update weights\n",
    "        gradients_discriminator = tape_disc.gradient(\n",
    "            target=discriminator_loss, \n",
    "            sources=self.discriminator.trainable_variables\n",
    "        )\n",
    "        self.discriminator_optimizer.apply_gradients(\n",
    "            zip(gradients_discriminator, self.discriminator.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        gradients_generator = tape_gen.gradient(\n",
    "            target=generator_loss, \n",
    "            sources=self.generator.trainable_variables\n",
    "        )\n",
    "        self.generator_optimizer.apply_gradients(\n",
    "            zip(gradients_generator, self.generator.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # update metrics\n",
    "        self.train_discriminator_loss.update_state(discriminator_loss)\n",
    "        self.train_generator_loss.update_state(generator_loss)\n",
    "        self.train_cheater_accuracy.update_state(\n",
    "            y_true=tf.ones_like(y_fake_logits),\n",
    "            y_pred=tf.nn.sigmoid(y_fake_logits)\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'generator_loss': self.train_generator_loss.result(), \n",
    "            'discriminator_loss': self.train_discriminator_loss.result(),\n",
    "            'cheater_accuracy': self.train_cheater_accuracy.result()\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        X_true, _ = data\n",
    "        batch_size = tf.shape(X_true)[0]\n",
    "        y_fake = tf.random.uniform([batch_size], minval=0, maxval=10, dtype=tf.int32)\n",
    "        # generate fake images\n",
    "        X_fake = self.call(y_fake, training=False)\n",
    "        # compute losses\n",
    "        generator_loss = self.compute_generator_loss(X_fake, y_fake)\n",
    "        discriminator_loss, y_fake_logits = self.compute_discriminator_loss(X_true, X_fake)\n",
    "        # update metrics\n",
    "        self.test_discriminator_loss.update_state(discriminator_loss)\n",
    "        self.test_generator_loss.update_state(generator_loss)\n",
    "        self.test_cheater_accuracy.update_state(\n",
    "            y_true=tf.ones_like(y_fake_logits),\n",
    "            y_pred=tf.nn.sigmoid(y_fake_logits)\n",
    "        )\n",
    "        return {\n",
    "            'generator_loss': self.test_generator_loss.result(), \n",
    "            'discriminator_loss': self.test_discriminator_loss.result(),\n",
    "            'cheater_accuracy': self.test_cheater_accuracy.result()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacbf670-4c4c-4b27-9114-a806bd5e0bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 700\n",
    "\n",
    "cc_gan_model = ConditionalClassifierGANModel(\n",
    "    generator_model=build_simple_generator_model(),\n",
    "    discriminator_model=build_simple_discriminator_model(),\n",
    "    classifier_model=classifier_model\n",
    ")\n",
    "cc_gan_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368cf585-728e-481d-ac72-4b070cbb8690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_EPOCHS = 300\n",
    "\n",
    "# cc_gan_model = ConditionalClassifierGANModel(\n",
    "#     generator_model=build_advanced_generator_model(),\n",
    "#     discriminator_model=build_advanced_discriminator_model(),\n",
    "#     classifier_model=classifier_model\n",
    "# )\n",
    "# cc_gan_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bf7cfa-1c53-45ae-b1f0-7957d4e12b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_gan_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0315b7-666b-4567-847e-93db092ade2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NUM_EPOCHS = 300\n",
    "\n",
    "train_history = cc_gan_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    verbose=1\n",
    ")\n",
    "train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ef62de-6ff7-4897-90c5-fda90e06f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images given targets\n",
    "fake_targets = tf.constant([5, 5, 5, 9, 9, 9, 1, 1, 1, 1, 3, 3, 3, 4, 4, 4])\n",
    "fake_images = cc_gan_model(fake_targets)\n",
    "fake_images = tf.clip_by_value(fake_image, 0, 1)\n",
    "display_images(fake_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ec194c-c7c2-4caa-b6ad-ac47d478048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_loss_plots(train_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42864eed-bec3-4894-9d2f-7496c77a2bb5",
   "metadata": {},
   "source": [
    "### Conditional GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a824ec7-a788-41da-88c2-e868c3b9c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_discriminator_model():\n",
    "    # inputs\n",
    "    inputs_image = layers.Input(shape=(8, 8, 1))\n",
    "    inputs_target = layers.Input(shape=(1,))\n",
    "    # discriminator\n",
    "    x = layers.CategoryEncoding(num_tokens=NUM_CLASSES, output_mode='one_hot')(inputs_target)\n",
    "    x = layers.Concatenate()([layers.Flatten()(inputs_image), x])\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    # output\n",
    "    output = layers.Dense(1)(x)\n",
    "    return models.Model(\n",
    "        inputs=[inputs_image, inputs_target], \n",
    "        outputs=output,\n",
    "        name='discriminator'\n",
    "    )\n",
    "    \n",
    "\n",
    "def build_advanced_discriminator_model():\n",
    "    # inputs\n",
    "    inputs_image = layers.Input(shape=(8, 8, 1))\n",
    "    inputs_target = layers.Input(shape=(1,))\n",
    "    # refactor target input\n",
    "    x_target = layers.CategoryEncoding(num_tokens=10, output_mode=\"one_hot\")(inputs_target)\n",
    "    x_target = layers.Dense(32, activation='relu')(x_target)\n",
    "    # discriminator\n",
    "    x = layers.Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\")(inputs_image)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Concatenate()([layers.Flatten()(x), x_target])\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    # output\n",
    "    output = layers.Dense(1)(x)\n",
    "    return models.Model(\n",
    "        inputs=[inputs_image, inputs_target], \n",
    "        outputs=output,\n",
    "        name='discriminator'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a47617-2eee-4db7-9413-25e674bfdfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGANModel(Model):\n",
    "\n",
    "    def __init__(self, generator_model, discriminator_model, **kwargs):\n",
    "        super().__init__(**kwargs) \n",
    "        # models\n",
    "        self.generator = generator_model\n",
    "        self.discriminator = discriminator_model\n",
    "        # optimizers\n",
    "        self.generator_optimizer = optimizers.Adam(1e-4)\n",
    "        self.discriminator_optimizer = optimizers.Adam(1e-4)\n",
    "        # metrics\n",
    "        self.train_generator_loss = metrics.Mean(name='train_generator_loss')\n",
    "        self.train_discriminator_loss = metrics.Mean(name='train_discriminator_loss')\n",
    "        self.test_generator_loss = metrics.Mean(name='test_generator_loss')\n",
    "        self.test_discriminator_loss = metrics.Mean(name='test_discriminator_loss')\n",
    "        self.train_cheater_accuracy = metrics.BinaryAccuracy(name='train_accuracy')\n",
    "        self.test_cheater_accuracy = metrics.BinaryAccuracy(name='test_accuracy')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        y_fake = inputs\n",
    "        noise = tf.random.normal([tf.shape(y_fake)[0], NOISE_DIM])\n",
    "        X_fake = self.generator([noise, y_fake])\n",
    "        return X_fake\n",
    "\n",
    "    def compute_discriminator_loss(self, X_true, X_fake, y_true, y_fake):\n",
    "        y_true_logits = self.discriminator([X_true, y_true])\n",
    "        y_fake_logits = self.discriminator([X_fake, y_fake])\n",
    "        loss_true = losses.binary_crossentropy(\n",
    "            y_true=tf.ones_like(y_true_logits), \n",
    "            y_pred=y_true_logits, \n",
    "            from_logits=True\n",
    "        )\n",
    "        loss_fake = losses.binary_crossentropy(\n",
    "            y_true=tf.zeros_like(y_fake_logits), \n",
    "            y_pred=y_fake_logits,\n",
    "            from_logits=True\n",
    "        )\n",
    "        total_loss = (loss_true + loss_fake) / 2.0\n",
    "        return tf.reduce_mean(total_loss), y_fake_logits\n",
    "    \n",
    "    def compute_generator_loss(self, X_fake, y_fake):\n",
    "        y_fake_logits = self.discriminator([X_fake, y_fake])\n",
    "        loss = losses.binary_crossentropy(\n",
    "            y_true=tf.ones_like(y_fake_logits), \n",
    "            y_pred=y_fake_logits, \n",
    "            from_logits=True\n",
    "        )\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        \n",
    "        # unpack input data\n",
    "        X_true, y_true = data\n",
    "        \n",
    "        with (\n",
    "            tf.GradientTape() as tape_gen, \n",
    "            tf.GradientTape() as tape_disc\n",
    "        ):\n",
    "            # number of images to generate\n",
    "            batch_size = tf.shape(X_true)[0]\n",
    "            # generate y for fake images\n",
    "            y_fake = tf.random.uniform([batch_size], minval=0, maxval=10, dtype=tf.int32)\n",
    "            # generate fake images\n",
    "            X_fake = self.call(y_fake, training=True)\n",
    "            # compute losses\n",
    "            discriminator_loss, y_fake_logits = self.compute_discriminator_loss(\n",
    "                X_true=X_true, \n",
    "                X_fake=X_fake, \n",
    "                y_true=y_true, \n",
    "                y_fake=y_fake\n",
    "            )\n",
    "            generator_loss = self.compute_generator_loss(X_fake, y_fake)\n",
    "\n",
    "        # compute gradients and update weights\n",
    "        gradients_discriminator = tape_disc.gradient(\n",
    "            target=discriminator_loss, \n",
    "            sources=self.discriminator.trainable_variables\n",
    "        )\n",
    "        self.discriminator_optimizer.apply_gradients(\n",
    "            zip(gradients_discriminator, self.discriminator.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        gradients_generator = tape_gen.gradient(\n",
    "            target=generator_loss, \n",
    "            sources=self.generator.trainable_variables\n",
    "        )\n",
    "        self.generator_optimizer.apply_gradients(\n",
    "            zip(gradients_generator, self.generator.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # update metrics\n",
    "        self.train_discriminator_loss.update_state(discriminator_loss)\n",
    "        self.train_generator_loss.update_state(generator_loss)\n",
    "        self.train_cheater_accuracy.update_state(\n",
    "            y_true=tf.ones_like(y_fake_logits),\n",
    "            y_pred=tf.nn.sigmoid(y_fake_logits)\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'generator_loss': self.train_generator_loss.result(), \n",
    "            'discriminator_loss': self.train_discriminator_loss.result(),\n",
    "            'cheater_accuracy': self.train_cheater_accuracy.result()\n",
    "        }\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        X_true, y_true = data\n",
    "        batch_size = tf.shape(X_true)[0]\n",
    "        y_fake = tf.random.uniform([batch_size], minval=0, maxval=10, dtype=tf.int32)\n",
    "        # generate fake images\n",
    "        X_fake = self.call(y_fake, training=False)\n",
    "        # compute losses\n",
    "        generator_loss = self.compute_generator_loss(X_fake, y_fake)\n",
    "        discriminator_loss, y_fake_logits = self.compute_discriminator_loss(X_true, X_fake, y_true, y_fake)\n",
    "        # update metrics\n",
    "        self.test_discriminator_loss.update_state(discriminator_loss)\n",
    "        self.test_generator_loss.update_state(generator_loss)\n",
    "        self.test_cheater_accuracy.update_state(\n",
    "            y_true=tf.ones_like(y_fake_logits),\n",
    "            y_pred=tf.nn.sigmoid(y_fake_logits)\n",
    "        )\n",
    "        return {\n",
    "            'generator_loss': self.test_generator_loss.result(), \n",
    "            'discriminator_loss': self.test_discriminator_loss.result(),\n",
    "            'cheater_accuracy': self.test_cheater_accuracy.result()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d912e1db-5d20-4cff-a1f9-3d9da060e558",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 700\n",
    "\n",
    "c_gan_model = ConditionalGANModel(\n",
    "    generator_model=build_simple_generator_model(),\n",
    "    discriminator_model=build_simple_discriminator_model()\n",
    ")\n",
    "c_gan_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3fb24e-4805-4b4e-b5c2-861d0c6e1548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_EPOCHS = 500\n",
    "\n",
    "# c_gan_model = ConditionalGANModel(\n",
    "#     generator_model=build_advanced_generator_model(),\n",
    "#     discriminator_model=build_advanced_discriminator_model()\n",
    "# )\n",
    "# c_gan_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4896ce7-48cd-4979-93af-d3feee082faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_gan_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f986e70-fafd-428b-b48e-961be65e5246",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NUM_EPOCHS = 300\n",
    "\n",
    "train_history = c_gan_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    verbose=1 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf3467b-92c9-44bd-a32b-dced76003af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images given targets\n",
    "fake_targets = tf.constant([5, 5, 5, 9, 9, 9, 1, 1, 1, 1, 3, 3, 3, 4, 4, 4])\n",
    "fake_images = cc_gan_model(fake_targets)\n",
    "fake_images = tf.clip_by_value(fake_image, 0, 1)\n",
    "display_images(fake_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d498bd2-d955-423e-8437-71fc69a5a0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_loss_plots(train_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1820db88-a9ea-4280-ac59-2528b4e5afa0",
   "metadata": {},
   "source": [
    "Generated images are mostly identical within the given target. How to solve this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f72932-9fc0-4e27-acc8-95f03d7d6fad",
   "metadata": {},
   "source": [
    "### Generating image based on text prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10a3bb2-90ac-4b5a-800a-4f306de88bd2",
   "metadata": {},
   "source": [
    "#### Loading word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb28a55-28ff-4d6a-a3af-66a3dd26fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(path_to_file):\n",
    "    \"\"\"Load words and their weights from file.\"\"\"\n",
    "    words = list()\n",
    "    embeddings = list()\n",
    "    with open(path_to_file) as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "            words.append(word)\n",
    "            embeddings.append(coefs)\n",
    "    return np.array(words), np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe35a4b-5fd1-4807-9dac-28c8ab87561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "FILEPATH = f'/media/sf_practice/data/debug_glove/glove.6B/glove.6B.{EMBEDDING_DIM}d.txt'\n",
    "\n",
    "# Load words and their embeddings\n",
    "words, embeddings = load_vectors(FILEPATH)\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae80d05-6bb9-4a7f-a598-183b8504ca4c",
   "metadata": {},
   "source": [
    "#### Embeddings for digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba9420-f256-48a9-939c-2ff033df4303",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a8701-50cc-4135-86e2-e7cbce8791c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_digits = np.zeros((NUM_CLASSES, EMBEDDING_DIM), dtype='float32')\n",
    "for index, label in enumerate(labels):\n",
    "    E_digits[index] = embeddings[np.isin(words, label, assume_unique=True)]\n",
    "\n",
    "E_digits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace6e12b-4af2-4d87-87a9-3c282ee78412",
   "metadata": {},
   "source": [
    "#### Target decoder as layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13722dd-fd5b-412a-8048-c21822545759",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetDecoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self, E_digits):\n",
    "        super().__init__()\n",
    "        self.E_digits = self.add_weight(\n",
    "            initializer=tf.constant_initializer(E_digits),\n",
    "            trainable=False,\n",
    "            dtype=tf.float32,\n",
    "            shape=E_digits.shape,\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        self.E_digits\n",
    "        target = tf.math.argmax(\n",
    "            tf.matmul(\n",
    "                # inputs, \n",
    "                # E_digits, \n",
    "                tf.math.l2_normalize(inputs, axis=1), \n",
    "                tf.math.l2_normalize(E_digits, axis=1), \n",
    "                transpose_b=True\n",
    "            ), axis=-1\n",
    "        )\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1a9cec-62f8-4d3d-b285-2a5893127ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize target decoder\n",
    "target_decoder = TargetDecoder(E_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3996c3af-1466-4916-8427-aa5c629a5565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode digit target from word\n",
    "Q = embeddings[np.argmax(words == 'five')][np.newaxis, :]\n",
    "target_decoder(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a2699a-040b-467c-af02-61a3e1d1f70c",
   "metadata": {},
   "source": [
    "#### Decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc214150-592e-47d7-8bd4-0d0dc76a58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TEXT_LENGTH = 20\n",
    "NUM_FEATURES = len(words) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da0ef95-9647-4aba-9eb5-a2943508e7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights of embedding laye\n",
    "E = np.zeros((NUM_FEATURES, EMBEDDING_DIM))\n",
    "# E[1] = np.random.normal(0, 0.1, EMBEDDING_DIM) # [UNK]\n",
    "E[2:] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2febd04b-f2af-4d8e-8418-7e21e96b9453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vectorizer_layer():\n",
    "    # setup vectorizer layer\n",
    "    vectorizer_layer = layers.TextVectorization(\n",
    "        max_tokens=NUM_FEATURES, \n",
    "        output_sequence_length=MAX_TEXT_LENGTH,\n",
    "        output_mode=\"int\"\n",
    "    )\n",
    "    # set vocabulary\n",
    "    vectorizer_layer.set_vocabulary(words)\n",
    "    return vectorizer_layer\n",
    "\n",
    "\n",
    "def build_embedding_layer():\n",
    "    # setup embedding layer\n",
    "    embedding_layer = layers.Embedding(\n",
    "        input_dim=NUM_FEATURES,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        trainable=False  # disable training\n",
    "    )\n",
    "    # initialize weights\n",
    "    embedding_layer.build((1, ))\n",
    "    # set weights\n",
    "    embedding_layer.set_weights([E])\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895f1a88-a0d7-4c3d-84be-73d071625203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_encoder_on_glove():\n",
    "    model = models.Sequential()\n",
    "    model.add(build_vectorizer_layer())\n",
    "    model.add(build_embedding_layer())\n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "    model.add(TargetDecoder(E_digits))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b06173-2106-4fcd-8a88-432c02dcaca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text decoder\n",
    "text_decoder = build_text_encoder_on_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42baa9b7-6e4d-4a49-8e69-38308992f284",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = tf.constant([\n",
    "    'Generate digit two', \n",
    "    'five', \n",
    "    'First digit'\n",
    "])\n",
    "\n",
    "# Decode texts\n",
    "targets = text_decoder(texts)\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aed1f19-da14-45a7-9565-80861860b09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images\n",
    "generated_images = c_gan_model(targets)\n",
    "display_images(generated_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ca3c8f-487f-4e01-b61d-6a87abfe9af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDigitGenerator:\n",
    "\n",
    "    def __init__(self, text_decoder_model, image_generator_model):\n",
    "        self.text_decoder = text_decoder_model\n",
    "        self.image_generator = image_generator_model\n",
    "\n",
    "    def generate(self, texts):\n",
    "        targets = self.text_decoder(texts)\n",
    "        return self.image_generator(targets)\n",
    "\n",
    "    def generate_and_display(self, texts):\n",
    "        generated_images = self.generate(texts)\n",
    "        display_images(generated_images)\n",
    "        return generated_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001192ee-fca4-41b5-b5db-4c76bee004b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_generator = ImageDigitGenerator(\n",
    "    text_decoder_model=text_decoder,\n",
    "    image_generator_model=c_gan_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e159d3-50fd-4a72-b420-891d5a41711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = image_generator.generate(texts)\n",
    "display_images(generated_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c176b3b-01d0-417e-b57f-2fb5cc1ecf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = image_generator.generate_and_display(texts)\n",
    "generated_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25841710-0a54-4a42-b9d2-7e85be13d9ad",
   "metadata": {},
   "source": [
    "## Stable Diffusion Model (Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3242de-6c3a-419d-92a9-154fb26b13aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade keras-cv tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419004fc-b773-4308-9173-e3f7c645d1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64f8dea-4987-4cec-92f3-b512921dac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable diffusion v1.5 (it downloads weights on first call)\n",
    "model = keras_cv.models.StableDiffusion(\n",
    "    img_width=512,\n",
    "    img_height=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb72546-adab-452a-842d-ee388fc5cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"photograph of an astronaut riding a horse\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dbb480-de70-46b0-8673-0500120dbafc",
   "metadata": {},
   "source": [
    "**Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99413672-a971-4370-95d9-65521d84f544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images\n",
    "images = model.text_to_image(\n",
    "    prompt=prompt,\n",
    "    batch_size=3,\n",
    "    num_steps=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf29a79-258c-4a2d-aa6d-6141c8839868",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9db922c-a8f0-4e61-8e5c-430c46c62bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f44beb-48ad-4a80-a081-3ff6f92d0e00",
   "metadata": {},
   "source": [
    "**Separate steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09495f03-4fd3-426c-a0a0-e6a9f658eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_encoded = model.encode_text(prompt)\n",
    "prompt_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8b18a-b7fe-4490-8f4a-dc9a16800436",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = model.generate_image(\n",
    "    encoded_text=prompt_encoded,\n",
    "    batch_size=1,\n",
    "    num_steps=20,\n",
    "    unconditional_guidance_scale=7.5,\n",
    "    negative_prompt=None\n",
    ")\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9968ae-1920-49b5-95b6-a4f640ba5387",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a3d07e-7f6b-4596-9e56-7702e89b8da6",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da71f74-1405-439c-a35f-9c8666a7cb87",
   "metadata": {},
   "source": [
    "- [Conditional GAN](https://keras.io/examples/generative/conditional_gan/)\n",
    "- [High-performance image generation using Stable Diffusion in KerasCV](https://www.tensorflow.org/tutorials/generative/generate_images_with_stable_diffusion)\n",
    "- [Stable Diffusion 3 in KerasHub!](https://keras.io/keras_hub/guides/stable_diffusion_3_in_keras_hub/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcc8be3-1bb9-4d09-87fb-d32e99f71553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
