{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f27a42e-1cfa-4a78-91a6-12bb1d86f709",
   "metadata": {},
   "source": [
    "# Image Recognition\n",
    "---\n",
    "\n",
    "S.Yu. Papulin (papulin.study@yandex.ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bc153f-5f56-4a63-bc79-af94825bb742",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "- [Loading Dataset](#Loading-Dataset)\n",
    "- [Preparing Dataset](#Preparing-Dataset)\n",
    "- [Building And Fitting Model](#Building-And-Fitting-Model)\n",
    "- [Evaluating Model](#Evaluating-Model)\n",
    "- [Saving And Loading Model](#Saving-And-Loading-Model)\n",
    "- [Sources](#Sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e098c-76d9-460b-aaad-649cb84aa549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import (\n",
    "    layers, \n",
    "    models, \n",
    "    Model, \n",
    "    utils, \n",
    "    losses, \n",
    "    optimizers, \n",
    "    metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28457c72-0a67-49ae-af4f-53c6c90c3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc999bc5-ffef-4e80-9796-e8b93b887215",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4836c2-04ab-42c8-b51a-42ed863a75a6",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e898675b-6b70-4374-9448-c846f112b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and show shape of data\n",
    "(X_trainval, y_trainval), (X_test, y_test) = cifar10.load_data()\n",
    "X_trainval.shape, y_trainval.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bff2c8-38d2-4d9e-a6b1-664840d2dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image value range\n",
    "X_trainval.max(), X_trainval.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec830be2-0db9-4d79-822b-42c5d1307de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique targets and their counts\n",
    "np.unique(y_trainval, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66e8565-7a89-4908-a908-1c4d6945e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First n targets\n",
    "y_trainval[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982c99bd-8bdb-481c-92d7-afc077d57521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class labels\n",
    "labels = np.array([\n",
    "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
    "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e41cc72-d9f1-4c7e-8931-35ec0c11e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(labels)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb44c45a-a2a4-4413-af2c-ff5397028d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 10 random images of each class\n",
    "NUM_DISPLAY_IMAGES = 10\n",
    "for target in range(num_classes):\n",
    "    indices = np.asarray(y_trainval==target).nonzero()[0]\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    indices_rnd = np.random.choice(indices, NUM_DISPLAY_IMAGES, replace=False)\n",
    "    print(f'Class label: {labels[target]}')\n",
    "    plt.figure(figsize=[10, 10])\n",
    "    for i in range(NUM_DISPLAY_IMAGES):\n",
    "        plt.subplot(1, NUM_DISPLAY_IMAGES, i+1)\n",
    "        plt.title(indices_rnd[i])\n",
    "        plt.imshow(X_trainval[indices_rnd[i]])\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628feca9-8c63-4f62-8d3a-4f4a96218743",
   "metadata": {},
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375c2700-4201-4972-a862-d75ea788e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose train and validation subsets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, \n",
    "    y_trainval, \n",
    "    test_size=0.1, \n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3783106a-90e7-48fe-aadd-11c8b67e77c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753faa11-6fb3-4dfe-a238-339d3cfe947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tf_dataset(X, y, batch_size=64, use_one_hot=False):\n",
    "    X = X.astype('float32') / 255.0\n",
    "    if use_one_hot:\n",
    "        y = utils.to_categorical(y)\n",
    "    else:\n",
    "        y = y.flatten()\n",
    "    return (\n",
    "        tf.data.Dataset.from_tensor_slices((X, y))\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "\n",
    "def print_first_batch(ds):\n",
    "    for X_batch, y_batch in ds.take(1):\n",
    "        print(X_batch)\n",
    "        print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4699c6-27b5-4327-9bc3-9d674c0a85ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = convert_to_tf_dataset(X_train, y_train)\n",
    "val_ds = convert_to_tf_dataset(X_val, y_val)\n",
    "test_ds = convert_to_tf_dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1a314f-4571-472d-9908-e28e3fb68b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_first_batch(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94839aeb-3556-4f89-9636-ac897ef8e04e",
   "metadata": {},
   "source": [
    "## Building And Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f410945-d2b9-406c-84d3-78b22429285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.saving import register_keras_serializable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8796748-bb1d-4fb3-bea5-0ec5092e0d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()\n",
    "class TinyConvModel(Model):\n",
    "\n",
    "    def __init__(self, num_classes=10, input_shape=(32, 32, 3), dropout_rate=0.1, **kwargs):\n",
    "        # self.input_layer = layers.Input(shape=input_shape)\n",
    "        self.layer_1 = layers.Conv2D(\n",
    "            filters=16, \n",
    "            kernel_size=(3, 3), \n",
    "            activation='relu', \n",
    "            padding='same',\n",
    "            name='conv1'\n",
    "        )\n",
    "        self.transform_1 = layers.MaxPooling2D((2, 2))\n",
    "        self.layer_2 = layers.Conv2D(32, (3, 3), activation='relu', padding='same', name='conv2')\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.transform_2 = layers.Flatten()\n",
    "        self.layer_3 = layers.Dense(128, activation='relu')\n",
    "        self.classifier = layers.Dense(num_classes)\n",
    "\n",
    "        # Note: We initialize with a given input shape so that\n",
    "        # we can later get a computation graph for specific layers\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        outputs = self.call(inputs)\n",
    "        super().__init__(inputs=inputs, outputs=outputs, **kwargs)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.layer_1(inputs)\n",
    "        x = self.transform_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.transform_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "def build_tiny_conv_model():\n",
    "    model = models.Sequential(name=\"ConvNet\")\n",
    "    model.add(layers.Input(shape=(32, 32, 3)))\n",
    "    model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='conv1'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), name='transform_1'))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', name=\"conv2\"))\n",
    "    model.add(layers.Dropout(0.1, name='dropout'))\n",
    "    model.add(layers.Flatten(name='transform_2'))\n",
    "    model.add(layers.Dense(128, activation='relu', name='layer_3'))\n",
    "    model.add(layers.Dense(10, name='layer_4'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b3fc96-52d3-4d7b-b56c-96c676b1d3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Another way to initialize a model graph\n",
    "# def init_model_inputs(model, input_shape):\n",
    "#     input_layer = layers.Input(shape=input_shape)\n",
    "#     return Model(inputs=input_layer, outputs=model(dummy_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcf6cf7-ee3d-4c1a-ab9c-7ffcf7ec01c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyConvModel()\n",
    "# model = build_tiny_conv_model()\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-3), \n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[metrics.SparseCategoricalAccuracy(),]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d570cc5c-2f0c-4dce-9edd-151547ed8f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: When we use one-hot representation of target\n",
    "# model.compile(\n",
    "#     optimizer=optimizers.Adam(learning_rate=1e-4), \n",
    "#     loss=losses.CategoricalCrossentropy(from_logits=True),\n",
    "#     metrics=[metrics.CategoricalAccuracy(),]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0c702a-3900-48b5-abd1-11ecb44f654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a85c2e2-9b27-4371-98db-f01b520b2430",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "\n",
    "train_history = model.fit(\n",
    "    train_ds,\n",
    "    # validation_split=0.1,\n",
    "    validation_data=val_ds,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce639409-c6fd-4cc1-aeb0-8a77d94e3504",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[14, 4])\n",
    "\n",
    "epochs = np.arange(1, len(train_history.history['loss'])+1)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Train vs val loss')\n",
    "plt.plot(epochs[1:], train_history.history['loss'][1:], '-og', label='train')\n",
    "plt.plot(epochs[1:], train_history.history['val_loss'][1:], \"-o\", color='orange', label='val')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Train vs val accuracy')\n",
    "plt.plot(epochs[1:], train_history.history['sparse_categorical_accuracy'][1:], '-og', label='train')\n",
    "plt.plot(epochs[1:], train_history.history['val_sparse_categorical_accuracy'][1:], '-o', color='orange', label='val')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67806b84-0ef6-415a-8408-2bc564e4c6b2",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f4166c-ffac-4e6e-b349-fd6d25a03605",
   "metadata": {},
   "source": [
    "##### Accuracy in test subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da21f7-14c6-40e3-9627-5cd332120bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_error = model.evaluate(test_ds)\n",
    "test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e44b09-045a-4255-943a-75f78fa5d1b0",
   "metadata": {},
   "source": [
    "##### Showing confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c0127c-2452-4a04-ba20-b9ca673e78aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba45b7-d3c5-4243-95a4-2072e6345e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test__logits = model.predict(test_ds.map(lambda image, target: image))\n",
    "y_test__pred = np.argmax(y_test__logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b8a7be-af4c-4dc5-b8aa-58761d53364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test__true = y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587acb43-0cab-48ff-b871-83f3e5151bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_true=y_test__true,\n",
    "    y_pred=y_test__pred,\n",
    "    labels=range(10),\n",
    "    display_labels=labels,\n",
    "    xticks_rotation='vertical'\n",
    ")\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e093e7a9-660f-476d-b4a8-edb57cc81dae",
   "metadata": {},
   "source": [
    "##### Single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a19e5-4a6a-4709-8e1e-53f70e4ffb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = X_test[6] / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f88e619-537b-45f4-a3f6-4ea7b56f8ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(test_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d34764-53d9-4c76-8386-0fd394f70022",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_batch = test_image[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb0bd2-83e6-4fad-9cb2-51bee451a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_batch = model.predict(test_image_batch)\n",
    "predictions_batch = np.argmax(logits_batch, axis=-1)\n",
    "predictions_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effc9ec9-12b2-494b-a130-1cf12872e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eb470f-8849-43aa-8963-bbd5de0c9a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_batch = softmax(logits_batch)\n",
    "probabilities_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ac311c-e6cc-415f-a575-ae57e2ff0af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[predictions_batch[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b0c27d-224a-4007-9a24-599caa12b68f",
   "metadata": {},
   "source": [
    "##### Batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedfa128-2fb8-4677-86f8-83288b566d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = X_test[:10] / 255.0\n",
    "test_targets = y_test[:10].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d6e03-75bc-473e-994e-33d7bc66fb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_batch = model.predict(test_images)\n",
    "test_pred = np.argmax(logits_batch, axis=-1)\n",
    "test_pred_labels = labels[test_pred]\n",
    "test_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33887d37-e713-49eb-b2d1-1ac4342f5217",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[14, 4])\n",
    "for index in range(len(test_images)):\n",
    "    plt.subplot(1, NUM_DISPLAY_IMAGES, index+1)\n",
    "    plt.title(\n",
    "        f'true: {labels[test_targets[index]]}\\npred: {test_pred_labels[index]}',\n",
    "        fontsize=10,\n",
    "        fontweight='normal'\n",
    "    )\n",
    "    plt.imshow(test_images[index])\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b535538-e091-4f90-9ea9-17ef2ce63490",
   "metadata": {},
   "source": [
    "**Filters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb3e45-4ecb-4c23-a815-f83bfcb712e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weigths_by_layer_index(model, index):\n",
    "    # get weights\n",
    "    W = model.layers[index].weights[0]\n",
    "    b = model.layers[index].weights[1]\n",
    "    # transponse to match imshow shape\n",
    "    W_T = tf.transpose(W, perm=[3, 0, 1, 2])\n",
    "    return W_T, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd0e201-1fae-4e20-82d9-de8284a6b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposed weights\n",
    "W_T_conv1, _ = get_weigths_by_layer_index(model, 1)\n",
    "W_T_conv2, _ = get_weigths_by_layer_index(model, 3)\n",
    "\n",
    "W_T_conv1.shape, W_T_conv2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfb4f32-f310-4558-8447-242541d1a703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_filters(W_T, num_per_row=10):\n",
    "    NUM_PER_ROW = num_per_row\n",
    "    num_images = W_T.shape[0]\n",
    "    num_rows = -(-num_images // NUM_PER_ROW)\n",
    "    plt.figure(figsize=[10, 1 * num_rows])\n",
    "    for index, image in enumerate(W_T):\n",
    "        plt.subplot(num_rows, NUM_PER_ROW, index+1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def display_all_filters(W_T):\n",
    "    for i in range(W_T.shape[3]):\n",
    "        display_filters(W_T[:, :, :, i], num_per_row=W_T.shape[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9397b54-2105-4c08-bf93-0f3b22fa3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv 1 filters\n",
    "display_all_filters(W_T_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cce627-1716-4d85-92eb-926684521387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv 2 filters\n",
    "display_all_filters(W_T_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3366c2f4-4cf1-4551-93b4-0211d201983f",
   "metadata": {},
   "source": [
    "**Convolutions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae8a0c5-4b5e-4c01-b2db-850e192f3c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(I):\n",
    "    NUM_PER_ROW = 10\n",
    "    num_images = I.shape[0]\n",
    "    num_rows = -(-num_images // NUM_PER_ROW)\n",
    "    plt.figure(figsize=[14, 1.5 * num_rows])\n",
    "    for index, image in enumerate(I):\n",
    "        plt.subplot(num_rows, NUM_PER_ROW, index+1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce5e560-4986-4533-a2e9-cdd724e76a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with intermediate conv layers\n",
    "intermediate_layer_model = Model(\n",
    "    inputs=model.input,\n",
    "    outputs=[\n",
    "        model.layers[1].output,\n",
    "        model.layers[3].output\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea99e87c-0eeb-429e-b320-d187fe05a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on a test image\n",
    "I_conv1, I_conv2 = intermediate_layer_model(test_image_batch)\n",
    "\n",
    "I_conv1.shape, I_conv2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9192da45-afa6-4a29-b728-0a3b5f501c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transponse to match imshow shape\n",
    "I_conv1 = tf.transpose(I_conv1, perm=[0, 3, 1, 2])\n",
    "I_conv2 = tf.transpose(I_conv2, perm=[0, 3, 1, 2])\n",
    "\n",
    "I_conv1.shape, I_conv2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73f9cfc-da70-4ced-bebb-0d03bb7f5931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv 1\n",
    "display_images(I_conv1[0, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57edbd37-ac54-48a7-b2ee-3a582e0a8300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv 2 \n",
    "display_images(I_conv2[0, :, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657341f7-f18c-4d6d-9bed-01ee7f8ac127",
   "metadata": {},
   "source": [
    "## Saving And Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f30b5df-8e80-44e9-9768-d0f9a97c916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_MODEL_PATH = '~/.keras/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a381d7-503e-40d9-9091-709820a9348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model as .keras\n",
    "model_filename = 'tiny_conv_net_128@10.keras'\n",
    "model_path = os.path.expanduser(os.path.join(BASE_MODEL_PATH, model_filename))\n",
    "print(f'Model path: {model_path}')\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9700de-fafb-42c1-bc0b-21e539ed24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_filename = 'tiny_conv_net_128@10.keras'\n",
    "model_path = os.path.expanduser(os.path.join(BASE_MODEL_PATH, model_filename))\n",
    "reconstructed_model = models.load_model(model_path)\n",
    "reconstructed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e9d89b-99da-4e5c-97b5-55703c141374",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_error = reconstructed_model.evaluate(test_ds)\n",
    "test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b602de1-6d3d-4a2c-a408-a12cdccff907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = reconstructed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39674fd2-7384-4180-8981-085e276a1dc2",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760c8528-a13b-40bf-944e-03e64e1466cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
