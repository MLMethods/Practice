{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAtO-axzxbdU"
   },
   "source": [
    "# Text classification using pretrained model\n",
    "---\n",
    "Sergei Papulin (papulin.study@yandex.ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **Warning.** It's much better to use `Colab` with `GPU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7NUgID8lNgw"
   },
   "outputs": [],
   "source": [
    "# For colab download and install packages\n",
    "# !pip install datasets==3.6.0 evaluate transformers==4.50.2 tensorflow tf_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zUMc1kZFu58K"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_33zV2QzI2V"
   },
   "outputs": [],
   "source": [
    "from datasets import (\n",
    "    DatasetDict,\n",
    "    Dataset,\n",
    "    ClassLabel,\n",
    "    Features,\n",
    "    Value,\n",
    "    concatenate_datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ENRTG3DtRFv"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XeIJrqRtZFO"
   },
   "outputs": [],
   "source": [
    "# Datasets\n",
    "\n",
    "def fetch_20news():\n",
    "  from sklearn.datasets import fetch_20newsgroups\n",
    "  # download if needed\n",
    "  dataset = fetch_20newsgroups(\n",
    "      subset=\"all\",\n",
    "      shuffle=True,\n",
    "      remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "      random_state=123\n",
    "  )\n",
    "  # create dictionary\n",
    "  arr_names = np.array(dataset.target_names)\n",
    "  # Note: We intentionally use names of classes instead of\n",
    "  # labels here to demonstate ClassLabel\n",
    "  dataset_dict = {'text': dataset.data, 'target': arr_names[dataset.target]}\n",
    "  ds = Dataset.from_dict(dataset_dict)\n",
    "  # convert target to ClassLabel\n",
    "  unique_target_names = sorted(list(set(ds['target'])))\n",
    "  target_feature = ClassLabel(names=unique_target_names)\n",
    "  features = Features({\n",
    "      **ds.features,\n",
    "      'target': target_feature  # replace existing target feature\n",
    "  })\n",
    "  ds = ds.cast(features)\n",
    "  # split data on train, val and test\n",
    "  ds = ds.train_test_split(test_size=0.3, stratify_by_column='target')\n",
    "  ds_trainval = ds['train'].train_test_split(test_size=0.1, stratify_by_column='target')\n",
    "  ds['train'] = ds_trainval['train']\n",
    "  ds['val'] = ds_trainval['test']\n",
    "  return ds\n",
    "\n",
    "\n",
    "def fetch_50authors():\n",
    "  import requests\n",
    "  from pathlib import Path\n",
    "  import zipfile\n",
    "  import io\n",
    "\n",
    "\n",
    "  def download_and_extract(url, path='downloaded_files'):\n",
    "      p = Path(path)\n",
    "      # if p.exists():\n",
    "      #     return path\n",
    "      response = requests.get(url, stream=True)\n",
    "      response.raise_for_status()\n",
    "      p.mkdir(exist_ok=True)\n",
    "      with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "          z.extractall(path)\n",
    "      return path\n",
    "\n",
    "\n",
    "  def gen_data(split_name='train', path='downloaded_files'):\n",
    "      for file_path in Path(path).glob(f\"C50{split_name}/*/*\"):\n",
    "          with open(file_path, mode='rt') as f:\n",
    "              yield {\n",
    "                  'target': file_path.parent.name,\n",
    "                  'text': f.read(),\n",
    "                  'filename': str(file_path)\n",
    "              }\n",
    "\n",
    "  # download data\n",
    "  DATA_URL = 'https://archive.ics.uci.edu/static/public/217/reuter+50+50.zip'\n",
    "  path = download_and_extract(url=DATA_URL)\n",
    "  # create generators\n",
    "  ds_train = Dataset.from_generator(\n",
    "    generator=gen_data,\n",
    "    gen_kwargs={'split_name': 'train', 'path': path},\n",
    "    features=Features({\n",
    "      'target': Value(dtype='string', id=None),\n",
    "      'text': Value(dtype='string', id=None)\n",
    "    })\n",
    "  )\n",
    "  ds_test = Dataset.from_generator(\n",
    "      generator=gen_data,\n",
    "      gen_kwargs={'split_name': 'test', 'path': path},\n",
    "      features=Features({\n",
    "        'target': Value(dtype='string', id=None),\n",
    "        'text': Value(dtype='string', id=None)\n",
    "      })\n",
    "  )\n",
    "  # convert target to ClassLabel\n",
    "  unique_target_names = sorted(list(set(ds_train['target'])))\n",
    "  target_feature = ClassLabel(names=unique_target_names)\n",
    "  features = Features({\n",
    "      **ds_train.features,\n",
    "      'target': target_feature  # replace existing target feature\n",
    "  })\n",
    "  ds_train = ds_train.cast(features)\n",
    "  ds_test = ds_test.cast(features)\n",
    "  # split data on train, val and test\n",
    "  ds = ds_train.train_test_split(test_size=0.1, stratify_by_column=\"target\")\n",
    "  ds_val = ds.pop(\"test\")\n",
    "  ds['val'] = ds_val\n",
    "  ds['test'] = ds_test\n",
    "  return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XC8j3eYWz_Pr"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "ds = fetch_20news()\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQ-O_8_i4kUI"
   },
   "outputs": [],
   "source": [
    "# Access set by name\n",
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohZr61Rk17DK"
   },
   "outputs": [],
   "source": [
    "# Access set elements by index\n",
    "ds['train'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATwPztB54gQ8"
   },
   "outputs": [],
   "source": [
    "# Access set element values by field name\n",
    "targets = ds['train'][:2]['target']\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7p4n90oX4y1G"
   },
   "outputs": [],
   "source": [
    "# reverse target to label\n",
    "ds['train'].features['target'].int2str(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47G6gl7j5uQg"
   },
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7BN76XI0uYD"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMr4fc6055Kb"
   },
   "outputs": [],
   "source": [
    "# Pretrained model name\n",
    "CHECKPOINT = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqZkXfCz0wk-"
   },
   "outputs": [],
   "source": [
    "# Load tokinezer associated with the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJ-lACYGAUrO"
   },
   "source": [
    "Single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvqPx4R38QAU"
   },
   "outputs": [],
   "source": [
    "# sample text\n",
    "text_input = 'If the function is asynchronous, then map will run your function in parallel'\n",
    "\n",
    "# tokenize\n",
    "tokenizer(text_input, padding='max_length', max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byjjegYO8kwM"
   },
   "outputs": [],
   "source": [
    "# encode text to ids\n",
    "text__encoded = tokenizer.encode(text_input)\n",
    "text__encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0AOTC6o8s_H"
   },
   "outputs": [],
   "source": [
    "# Ids to string\n",
    "tokenizer.decode(text__encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5MRRzezi9uaj"
   },
   "outputs": [],
   "source": [
    "# Tokens\n",
    "tokenizer.tokenize(text_input, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTyYqG7cAa4T"
   },
   "source": [
    "Entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5oPHoi80yfl"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example['text'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVOTYldC00sV"
   },
   "outputs": [],
   "source": [
    "ds__tokenized = ds.map(tokenize_function, batched=True)\n",
    "ds__tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHCeDrXE02et"
   },
   "outputs": [],
   "source": [
    "print(f\"input_ids: {ds__tokenized['train'][0]['input_ids'][:5]}\")\n",
    "print(f\"token_type_ids: {ds__tokenized['train'][0]['token_type_ids'][:5]}\")\n",
    "print(f\"attention_mask: {ds__tokenized['train'][0]['attention_mask'][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBayg-Cj06hV"
   },
   "outputs": [],
   "source": [
    "ds__tokenized['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWgk3A6D08Qs"
   },
   "outputs": [],
   "source": [
    "# putting together samples inside a batch is called a collate function\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D61yUI0JCPt6"
   },
   "outputs": [],
   "source": [
    "# example of use of collate function\n",
    "samples = ds__tokenized[\"train\"][:10]\n",
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v07pjXcGB_KW"
   },
   "outputs": [],
   "source": [
    "# create batches and convert to tensorflow dataset\n",
    "tf_train_dataset = ds__tokenized['train'].to_tf_dataset(\n",
    "    columns=['attention_mask', 'input_ids', 'token_type_ids'],\n",
    "    label_cols='target',\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "tf_val_dataset = ds__tokenized['val'].to_tf_dataset(\n",
    "    columns=['attention_mask', 'input_ids', 'token_type_ids'],\n",
    "    label_cols='target',\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "tf_test_dataset = ds__tokenized['test'].to_tf_dataset(\n",
    "    columns=['attention_mask', 'input_ids', 'token_type_ids'],\n",
    "    label_cols='target',\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "id7yapuN3o0O"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrJdD_F71EcQ"
   },
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tf_keras.optimizers import Adam\n",
    "from tf_keras.optimizers.schedules import PolynomialDecay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtE1x2MsmK5R"
   },
   "source": [
    "**BERT** (uncased):\n",
    "- params: `110M`\n",
    "- layers: `12`\n",
    "- heads: `12`\n",
    "- dimensitions: `768`\n",
    "- context length: `512`\n",
    "- tokenizer: `WordPiece`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCmPz6yGDIo9"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(ds['train'].features['target'].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbgQx56Q1d5P"
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    CHECKPOINT,\n",
    "    num_labels=NUM_CLASSES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4EpnsZ9DprP"
   },
   "outputs": [],
   "source": [
    "# model topology\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8az8B9CI2Gu"
   },
   "outputs": [],
   "source": [
    "model.num_parameters(), model.num_parameters(only_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ERqOsSeEKvy"
   },
   "outputs": [],
   "source": [
    "# iterate over layers\n",
    "for layer in model.layers:\n",
    "  print(f'{layer.name} {layer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOmR2mpREPAG"
   },
   "outputs": [],
   "source": [
    "# layers of BERT model\n",
    "for layer in model.bert.encoder.layer:\n",
    "  print(f'{layer.name} {layer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNMWZOO_EPhG"
   },
   "outputs": [],
   "source": [
    "# number of neurons of output layer\n",
    "model.classifier.units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mU1cpKDpFaI7"
   },
   "outputs": [],
   "source": [
    "# turn on/off weights from training (freezing)\n",
    "\n",
    "# trainable parameters\n",
    "print(f'Trainable parameters: {model.bert.trainable}')\n",
    "\n",
    "# freeze parameters\n",
    "model.bert.trainable = False\n",
    "print(f'Trainable parameters: {model.bert.trainable}')\n",
    "model.summary()\n",
    "\n",
    "# freeze only specific layers of BERT\n",
    "model.bert.trainable = True\n",
    "model.bert.embeddings.trainable = False\n",
    "model.bert.pooler.trainable = True\n",
    "for layer in model.bert.encoder.layer[:-1]:\n",
    "  layer.trainable = False\n",
    "print('\\nSome layers frozen\\n')\n",
    "model.summary()\n",
    "\n",
    "for layer in model.bert.encoder.layer:\n",
    "  print(f'Layer: {layer.name}, trainable: {layer.trainable}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_G2my-V6rlPr"
   },
   "source": [
    "## Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCbccA2SFyUO"
   },
   "outputs": [],
   "source": [
    "def get_optimizer(num_batches, batch_size, num_epochs):\n",
    "  num_train_steps = num_batches * num_epochs\n",
    "  lr_scheduler = PolynomialDecay(\n",
    "    initial_learning_rate=5e-5,\n",
    "    end_learning_rate=0.0,\n",
    "    decay_steps=num_train_steps\n",
    "  )\n",
    "  return Adam(learning_rate=lr_scheduler)\n",
    "\n",
    "\n",
    "def build_model(optimizer):\n",
    "  # load base model\n",
    "  model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "      CHECKPOINT,\n",
    "      num_labels=NUM_CLASSES\n",
    "  )\n",
    "  # setup trainable layers\n",
    "  model.bert.embeddings.trainable = False\n",
    "  model.bert.pooler.trainable = True\n",
    "  for layer in model.bert.encoder.layer[:-1]:\n",
    "    layer.trainable = False\n",
    "  # loss\n",
    "  loss = SparseCategoricalCrossentropy(from_logits=True)\n",
    "  # compile\n",
    "  model.compile(\n",
    "      optimizer=optimizer,\n",
    "      loss=loss,\n",
    "      metrics=[\"accuracy\"]\n",
    "  )\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDY9iZ1YqJ6V"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# setup model\n",
    "optimizer = get_optimizer(\n",
    "    num_batches=len(tf_train_dataset),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=NUM_EPOCHS\n",
    ")\n",
    "model = build_model(optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHbi2F4IrvCn"
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "history = model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_val_dataset,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTG4GGmWrug0"
   },
   "outputs": [],
   "source": [
    "# TODO: save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WE6lc8pirtiL"
   },
   "outputs": [],
   "source": [
    "# TODO: load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNFMtks8r3WQ"
   },
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1pOZB1DxPsH"
   },
   "outputs": [],
   "source": [
    "class_label = ds['train'].features['target']\n",
    "class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-maPc04vOBj"
   },
   "outputs": [],
   "source": [
    "def get_predictions(outputs):\n",
    "  logits = outputs['logits']\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqgl8G8mxoqd"
   },
   "source": [
    "Single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gsiwvQI5uFQ5"
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "text_input = 'If the function is asynchronous, then map will run your function in parallel'\n",
    "text__encoded = tokenizer(text_input, return_tensors=\"tf\")\n",
    "# Not recommended\n",
    "# text__encoded = tokenizer.encode(text_input)\n",
    "# text__encoded = tokenizer(text_input, return_tensors=\"tf\")['input_ids']\n",
    "\n",
    "outputs = model.predict(text__encoded)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BvjOxDjGwLce"
   },
   "outputs": [],
   "source": [
    "# class prediction\n",
    "predictions = get_predictions(outputs)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iROOy7xUwyCO"
   },
   "outputs": [],
   "source": [
    "# class label\n",
    "class_label.int2str(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5razxWfxyL5"
   },
   "source": [
    "Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7e3gManu2hKc"
   },
   "outputs": [],
   "source": [
    "# Option 1: Using TF Dataset with true label\n",
    "model.evaluate(tf_test_dataset.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LqbLPdvS2D6c"
   },
   "outputs": [],
   "source": [
    "# Option 2: Predictions \n",
    "test_true = np.array(ds__tokenized['test'][:8*5]['target'])\n",
    "test_pred = get_predictions(model.predict(tf_test_dataset.take(5)))\n",
    "\n",
    "# accuracy\n",
    "1/len(test_pred) * np.sum(test_pred == test_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face `evaluate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FO-DWBX0yCLm"
   },
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ou7xvO0TrtW2"
   },
   "outputs": [],
   "source": [
    "# load metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "print(accuracy.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZZ3WgJwx9pT"
   },
   "outputs": [],
   "source": [
    "# args\n",
    "accuracy.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMsS53MEx9yn"
   },
   "outputs": [],
   "source": [
    "# run\n",
    "accuracy.compute(references=test_true, predictions=test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "for inputs, targets in tf_test_dataset.take(5):\n",
    "    outputs = model.predict(inputs)\n",
    "    accuracy.add_batch(references=targets, predictions=get_predictions(outputs))\n",
    "accuracy.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xs3U8Oizr63P"
   },
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "metrics = evaluate.combine([\n",
    "    evaluate.load('precision'),\n",
    "    evaluate.load('recall'),\n",
    "    evaluate.load('f1')\n",
    "])\n",
    "\n",
    "for inputs, targets in tf_test_dataset.take(5):\n",
    "    outputs = model.predict(inputs)\n",
    "    metrics.add_batch(references=targets, predictions=get_predictions(outputs))\n",
    "metrics.compute(average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wN8wtDIJr7nM"
   },
   "source": [
    "## Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access to `BERT` outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take single batch of 8 items [0] and use only feature inputs [0]\n",
    "test_input_ids = list(tf_test_dataset.take(1))[0][0]\n",
    "test_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.bert(test_input_ids)\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last hidden state\n",
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOKeJ8WGYhb8"
   },
   "outputs": [],
   "source": [
    "# pooler_output\n",
    "outputs[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
