{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:18pt; padding-top:20px; text-align:center\">СЕМИНАР. <b>Регуляризация</b></div><hr>\n",
    "<div style=\"text-align:right;\">Папулин С.Ю. <span style=\"font-style: italic;font-weight: bold;\">(papulin.study@mail.ru)</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Содержание\n",
    "\n",
    "1. [Линейная полиномиальная регрессия](#1.-Линейная-полиномиальная-регрессия)\n",
    "2. [Регуляризация в линейной регрессии](#2.-Регуляризация-в-линейной-регрессии)\n",
    "3. [Выбор коэффициента регуляризации с кросс-валидацией](#3.-Выбор-коэффициента-регуляризации-с-кросс-валидацией)\n",
    "4. [Классификация с кросс-валидацией](#4.-Классификация-с-кросс-валидацией)\n",
    "5. [Источники](#5.-Источники)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подключение библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import (\n",
    "    Ridge, Lasso, RidgeCV, LassoCV,\n",
    "    LogisticRegression, LogisticRegressionCV, RidgeClassifier)\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Линейная полиномиальная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_dataset(n=100):\n",
    "    \"\"\"Генерация исходных данных\"\"\"\n",
    "    x = stats.uniform.rvs(size=n, loc=0, scale=6, random_state=0)\n",
    "    f = lambda x: np.sin(x)\n",
    "    y = stats.norm.rvs(size=n, loc=0, scale=0.5, random_state=0) + f(x)\n",
    "    return (x, y, f)\n",
    "\n",
    "\n",
    "# Инициализация исходных данных\n",
    "x, y, f = regression_dataset()\n",
    "\n",
    "# График\n",
    "xx = np.linspace(0, 6, 100)\n",
    "plt.plot(x, y, \"o\", color=\"green\", label=\"observed data\")\n",
    "plt.plot(xx, f(xx), \"-\", color=\"SteelBlue\", label=\"true function\")\n",
    "plt.title(\"Initial data\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры\n",
    "REGULARIZATION = 0\n",
    "POLY_DEGREE = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Представление признака x в матричной форме\n",
    "X_ = x.reshape(-1, 1)\n",
    "\n",
    "# Формирование обучающего и тестового подмножеств\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_, y, test_size=0.3, random_state=200)\n",
    "\n",
    "# Формирование последовательности действий\n",
    "pipeline = Pipeline([\n",
    "    (\"polynomizer\", PolynomialFeatures(degree=POLY_DEGREE, include_bias=False)), \n",
    "    (\"standardizer\", StandardScaler()),\n",
    "    (\"linear_model\", Ridge(alpha=REGULARIZATION, fit_intercept=True))\n",
    "])\n",
    "\n",
    "# Обучение\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Параметры обученной модели\n",
    "print(\"Параметры модели:\")\n",
    "print(\"\\tw{} = {}\".format(0, pipeline.named_steps[\"linear_model\"].intercept_))\n",
    "for indx, coef in enumerate(pipeline.named_steps[\"linear_model\"].coef_):\n",
    "    print(\"\\tw{} = {}\".format(indx+1, coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказания на обучающем и тестовом множествах\n",
    "y_train__pred = pipeline.predict(X_train)\n",
    "y_test__pred = pipeline.predict(X_test)\n",
    "\n",
    "# Ошибки на обучающем множестве\n",
    "mse_traint = mean_squared_error(y_train, y_train__pred)\n",
    "r2_train = r2_score(y_train, y_train__pred)\n",
    "\n",
    "print(\"Обучающее множество:\")\n",
    "print(\"\\tTrain MSE:\", mse_traint)\n",
    "print(\"\\tTrain R^2:\", r2_train)\n",
    "\n",
    "# Ошибки на тестовом множестве\n",
    "mse_test = mean_squared_error(y_test, y_test__pred)\n",
    "r2_test = r2_score(y_test, y_test__pred)\n",
    "\n",
    "print(\"\\nТестовое множество:\")\n",
    "print(\"\\tTest MSE:\", mse_test)\n",
    "print(\"\\tTest R^2:\", r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Графики\n",
    "xx = np.linspace(0, 6, 100).reshape(-1, 1)\n",
    "plt.figure(figsize=[12, 4])\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "plt.title(\"Train data\")\n",
    "plt.plot(X_train, y_train, \"o\", color=\"green\")\n",
    "plt.plot(xx, pipeline.predict(xx), color=\"red\", lw=2, label=\"predicted\")\n",
    "plt.scatter(X_train, y_train__pred, color=\"red\")\n",
    "plt.vlines(X_train, ymin=y_train, ymax=y_train__pred, colors=\"black\", linestyles=\"dotted\")\n",
    "plt.plot(xx, f(xx), \"-\", color=\"SteelBlue\", label=\"true function\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "\n",
    "plt.title(\"Test data\")\n",
    "plt.plot(X_test, y_test, \"o\", color=\"green\")\n",
    "plt.plot(xx, pipeline.predict(xx), color=\"red\", lw=2, label=\"predicted\")\n",
    "plt.scatter(X_test, y_test__pred, color=\"red\")\n",
    "plt.vlines(X_test, ymin=y_test, ymax=y_test__pred, colors=\"black\", linestyles=\"dotted\")\n",
    "plt.plot(xx, f(xx), \"-\", color=\"SteelBlue\", label=\"true function\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Регуляризация в линейной регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры\n",
    "REGULARIZATION = 0.001\n",
    "POLY_DEGREE = 15\n",
    "\n",
    "# Формирование последовательности действий с ридж-регрессией\n",
    "pipeline = Pipeline([\n",
    "    (\"polynomizer\", PolynomialFeatures(degree=POLY_DEGREE, include_bias=False)), \n",
    "    (\"standardizer\", StandardScaler()),\n",
    "    (\"linear_model\", Ridge(alpha=REGULARIZATION, fit_intercept=True))\n",
    "])\n",
    "\n",
    "# Формирование последовательности действий с лассо-регрессией\n",
    "# pipeline = Pipeline([\n",
    "#     (\"transformation\", PolynomialFeatures(degree=15)), \n",
    "#     (\"linear_model\", Lasso(alpha=REGULARIZATION, fit_intercept=False))\n",
    "# ])\n",
    "\n",
    "# Обучение\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Параметры обученной модели\n",
    "print(\"Параметры модели:\")\n",
    "print(\"\\tw{} = {}\".format(0, pipeline.named_steps[\"linear_model\"].intercept_))\n",
    "for indx, coef in enumerate(pipeline.named_steps[\"linear_model\"].coef_):\n",
    "    print(\"\\tw{} = {}\".format(indx+1, coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказания на обучающем и тестовом множествах\n",
    "y_train__pred = pipeline.predict(X_train)\n",
    "y_test__pred = pipeline.predict(X_test)\n",
    "\n",
    "# Ошибки на обучающем множестве\n",
    "mse_traint = mean_squared_error(y_train, y_train__pred)\n",
    "r2_train = r2_score(y_train, y_train__pred)\n",
    "\n",
    "print(\"Обучающее множество:\")\n",
    "print(\"\\tTrain MSE:\", mse_traint)\n",
    "print(\"\\tTrain R^2:\", r2_train)\n",
    "\n",
    "# Ошибки на тестовом множестве\n",
    "mse_test = mean_squared_error(y_test, y_test__pred)\n",
    "r2_test = r2_score(y_test, y_test__pred)\n",
    "\n",
    "print(\"\\nТестовое множество:\")\n",
    "print(\"\\tTest MSE:\", mse_test)\n",
    "\n",
    "\n",
    "print(\"\\tTest R^2:\", r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Графики\n",
    "xx = np.linspace(0, 6, 100).reshape(-1, 1)\n",
    "plt.figure(figsize=[12, 4])\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "plt.title(\"Train data\")\n",
    "plt.plot(X_train, y_train, \"o\", color=\"green\")\n",
    "plt.plot(xx, pipeline.predict(xx), color=\"red\", lw=2, label=\"predicted\")\n",
    "plt.scatter(X_train, y_train__pred, color=\"red\")\n",
    "plt.vlines(X_train, ymin=y_train, ymax=y_train__pred, colors=\"black\", linestyles=\"dotted\")\n",
    "plt.plot(xx, f(xx), \"-\", color=\"SteelBlue\", label=\"true function\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "\n",
    "plt.title(\"Test data\")\n",
    "plt.plot(X_test, y_test, \"o\", color=\"green\")\n",
    "plt.plot(xx, pipeline.predict(xx), color=\"red\", lw=2, label=\"predicted\")\n",
    "plt.scatter(X_test, y_test__pred, color=\"red\")\n",
    "plt.vlines(X_test, ymin=y_test, ymax=y_test__pred, colors=\"black\", linestyles=\"dotted\")\n",
    "plt.plot(xx, f(xx), \"-\", color=\"SteelBlue\", label=\"true function\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Выбор коэффициента регуляризации с кросс-валидацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация экземпляра класса для кросс-валидации\n",
    "kfolds = KFold(n_splits=5, shuffle=True, random_state=12345)\n",
    "\n",
    "# Исследуемые параметры регуляризации\n",
    "alphas = [0.0000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование `RidgeCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры ридж-регрессии\n",
    "ridge_args = {\n",
    "    \"alphas\": alphas,\n",
    "    \"fit_intercept\": True,\n",
    "    \"cv\": kfolds,\n",
    "    \"scoring\": \"neg_mean_squared_error\"\n",
    "}\n",
    "\n",
    "# Формирование последовательности действий с ридж-регрессией\n",
    "pipeline = Pipeline([\n",
    "    (\"polynomizer\", PolynomialFeatures(degree=POLY_DEGREE, include_bias=False)), \n",
    "    (\"standardizer\", StandardScaler()),\n",
    "    (\"linear_model\", RidgeCV(**ridge_args))\n",
    "])\n",
    "\n",
    "# Обучение\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Параметры обученной модели\n",
    "print(\"Параметры модели:\")\n",
    "print(\"\\tw{} = {}\".format(0, pipeline.named_steps[\"linear_model\"].intercept_))\n",
    "for indx, coef in enumerate(pipeline.named_steps[\"linear_model\"].coef_):\n",
    "    print(\"\\tw{} = {}\".format(indx+1, coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметр регуляризации лучшей модели\n",
    "pipeline.named_steps[\"linear_model\"].alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказания на обучающем и тестовом множествах\n",
    "y_train__pred = pipeline.predict(X_train)\n",
    "y_test__pred = pipeline.predict(X_test)\n",
    "\n",
    "# Ошибки на обучающем множестве\n",
    "mse_traint = mean_squared_error(y_train, y_train__pred)\n",
    "r2_train = r2_score(y_train, y_train__pred)\n",
    "\n",
    "print(\"Обучающее множество:\")\n",
    "print(\"\\tTrain MSE:\", mse_traint)\n",
    "print(\"\\tTrain R^2:\", r2_train)\n",
    "\n",
    "# Ошибки на тестовом множестве\n",
    "mse_test = mean_squared_error(y_test, y_test__pred)\n",
    "r2_test = r2_score(y_test, y_test__pred)\n",
    "\n",
    "print(\"\\nТестовое множество:\")\n",
    "print(\"\\tTest MSE:\", mse_test)\n",
    "print(\"\\tTest R^2:\", r2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование `GridSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Формирование последовательности действий с ридж-регрессией\n",
    "pipeline = Pipeline([\n",
    "    (\"polynomizer\", PolynomialFeatures(degree=POLY_DEGREE, include_bias=False)), \n",
    "    (\"standardizer\", StandardScaler()),\n",
    "    (\"linear_model\", Ridge(alpha=REGULARIZATION, fit_intercept=True))\n",
    "])\n",
    "\n",
    "# Сетка параметров\n",
    "parameters = {\n",
    "    \"linear_model__alpha\": alphas\n",
    "}\n",
    "\n",
    "# Параметры обучения\n",
    "grid_search_args = {\n",
    "    \"estimator\": pipeline,\n",
    "    \"param_grid\": parameters,\n",
    "    \"cv\": kfolds,\n",
    "    \"scoring\": \"neg_mean_squared_error\",\n",
    "    \"return_train_score\": True\n",
    "}\n",
    "\n",
    "\n",
    "# Обучение\n",
    "# Замечание: GridSearchCV после выбора параметров автоматически \n",
    "# заново обучается на всем множестве\n",
    "grid_search = GridSearchCV(**grid_search_args)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Параметры обученной модели\n",
    "print(\"Параметры модели:\")\n",
    "print(\"\\tw{} = {}\".format(0, grid_search.best_estimator_.named_steps[\"linear_model\"].intercept_))\n",
    "for indx, coef in enumerate(grid_search.best_estimator_.named_steps[\"linear_model\"].coef_):\n",
    "    print(\"\\tw{} = {}\".format(indx+1, coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вывод сведений по обучению\n",
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры лучшей модели\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказания на обучающем и тестовом множествах\n",
    "y_train__pred = grid_search.predict(X_train)\n",
    "y_test__pred = grid_search.predict(X_test)\n",
    "\n",
    "# Ошибки на обучающем множестве\n",
    "mse_traint = mean_squared_error(y_train, y_train__pred)\n",
    "r2_train = r2_score(y_train, y_train__pred)\n",
    "\n",
    "print(\"Обучающее множество:\")\n",
    "print(\"\\tTrain MSE:\", mse_traint)\n",
    "print(\"\\tTrain R^2:\", r2_train)\n",
    "\n",
    "# Ошибки на тестовом множестве\n",
    "mse_test = mean_squared_error(y_test, y_test__pred)\n",
    "r2_test = r2_score(y_test, y_test__pred)\n",
    "\n",
    "print(\"\\nТестовое множество:\")\n",
    "print(\"\\tTest MSE:\", mse_test)\n",
    "print(\"\\tTest R^2:\", r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# График выбора параметра регуляризации\n",
    "\n",
    "alphas = np.asarray(grid_search.cv_results_[\"param_linear_model__alpha\"], dtype=\"float\")\n",
    "\n",
    "plt.figure(figsize=[6, 4])\n",
    "\n",
    "plt.subplot(1,1,1)\n",
    "plt.title(\"Model Selection\")\n",
    "plt.plot(alphas, -grid_search.cv_results_[\"mean_test_score\"], \"o-\", label=\"Validation\")\n",
    "plt.plot(alphas, -grid_search.cv_results_[\"mean_train_score\"], \"o-\", label=\"Train\")\n",
    "plt.xlabel(\"$Regularization$\")\n",
    "plt.ylabel(\"$MSE$\")\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Графики функции предсказания\n",
    "\n",
    "xx = np.linspace(0, 6, 100).reshape(-1, 1)\n",
    "plt.figure(figsize=[12, 4])\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "plt.title(\"Train data\")\n",
    "plt.plot(X_train, y_train, \"o\", color=\"green\")\n",
    "plt.plot(xx, grid_search.predict(xx), color=\"red\", lw=2, label=\"predicted\")\n",
    "plt.scatter(X_train, y_train__pred, color=\"red\")\n",
    "plt.vlines(X_train, ymin=y_train, ymax=y_train__pred, colors=\"black\", linestyles=\"dotted\")\n",
    "plt.plot(xx, f(xx), \"-\", color=\"SteelBlue\", label=\"true function\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "\n",
    "plt.title(\"Test data\")\n",
    "plt.plot(X_test, y_test, \"o\", color=\"green\")\n",
    "plt.plot(xx, grid_search.predict(xx), color=\"red\", lw=2, label=\"predicted\")\n",
    "plt.scatter(X_test, y_test__pred, color=\"red\")\n",
    "plt.vlines(X_test, ymin=y_test, ymax=y_test__pred, colors=\"black\", linestyles=\"dotted\")\n",
    "plt.plot(xx, f(xx), \"-\", color=\"SteelBlue\", label=\"true function\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Классификация с кросс-валидацией"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор коэффициента регуляризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_INDX = 3\n",
    "\n",
    "print(\"Image matrix:\\n\", digits[\"images\"][IMAGE_INDX])\n",
    "print(\"Target value:\", digits.target[IMAGE_INDX])\n",
    "\n",
    "plt.imshow(digits.images[IMAGE_INDX])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры регуляризации\n",
    "Cs = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# Способ разделения данных\n",
    "kfolds = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Исходные данные для обучения\n",
    "X = digits[\"images\"].reshape(len(digits[\"images\"]), -1)\n",
    "y = digits[\"target\"]\n",
    "\n",
    "# Формирование обучающего и тестового подмножеств\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры инициализации модели\n",
    "model_args = {\n",
    "    \"penalty\": \"l2\", \n",
    "    \"Cs\": Cs,\n",
    "    \"cv\": kfolds, \n",
    "    \"multi_class\": \"multinomial\", \n",
    "    \"solver\": \"newton-cg\", \n",
    "    \"scoring\": \"accuracy\",\n",
    "    \"max_iter\": 200,\n",
    "    \"random_state\": 12345\n",
    "}\n",
    "\n",
    "# Обучение\n",
    "logistic_model = LogisticRegressionCV(**model_args)\n",
    "logistic_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры обученной модели\n",
    "for i in range(10):\n",
    "    print(\"Параметры модели {}:\".format(i+1))\n",
    "    print(\"\\tw{} = {}\".format(0, logistic_model.intercept_[i]))\n",
    "    for indx, coef in enumerate(logistic_model.coef_[i].flatten()):\n",
    "        print(\"\\tw{} = {}\".format(indx+1, coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Коэффициенты регуляризации\n",
    "logistic_model.C_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка качества модели\n",
    "train_error = logistic_model.score(X_train, y_train)\n",
    "test_error = logistic_model.score(X_test, y_test)\n",
    "\n",
    "print(\"Train Accuracy:\", train_error)\n",
    "print(\"Test Accuracy:\", test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "Сделать то же самое, что и до этого, но с `GridSearchCV` и построить график выбора параметра регуляризации. Другими словами, необходимо реализовать выбор коэффициента регуляризации (`C`) на обучающем множестве `train` для логистической регрессии из набора `Cs` с использованием `GridSearchCV` и `kfolds`. Построить графики значений `accuracy` для обучающего и проверочного множеств для каждого значения из `Cs`. При отображении на графике коэффициенты `Сs` представить в виде $log_{10}(C)$. После этого вычислить `accuracy` на множестве `train` и `test`. Замечание: все остальные параметры логистической регрессии оставить прежними, что и в предыдущей реализации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: GridSearchCV + график выбора модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сетка параметров\n",
    "parameters = {\n",
    "    \"C\": Cs\n",
    "}\n",
    "\n",
    "# Параметры модели\n",
    "model_class_parameters = {\n",
    "    \"penalty\": \"l2\", \n",
    "    \"C\": float(\"inf\"),\n",
    "    \"multi_class\": \"ovr\", \n",
    "    \"solver\": \"newton-cg\", \n",
    "    \"max_iter\": 200,\n",
    "    \"random_state\": 12345\n",
    "}\n",
    "\n",
    "logistic_model = LogisticRegression(**model_class_parameters)\n",
    "\n",
    "# Параметры сетки\n",
    "grid_class_parameters = {\n",
    "    \"estimator\": logistic_model, \n",
    "    \"param_grid\": parameters,\n",
    "    \"cv\": kfolds,\n",
    "    \"scoring\": \"accuracy\",\n",
    "    \"return_train_score\": True\n",
    "}\n",
    "\n",
    "# Обучение\n",
    "grid_search = GridSearchCV(**grid_class_parameters)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# График выбора параметра регуляризации\n",
    "\n",
    "plt.figure(figsize=[6, 4])\n",
    "\n",
    "plt.subplot(1,1,1)\n",
    "plt.title(\"Model Selection\")\n",
    "plt.plot(Cs, grid_search.cv_results_[\"mean_test_score\"], \"o-\", label=\"Validation\")\n",
    "plt.plot(Cs, grid_search.cv_results_[\"mean_train_score\"], \"o-\", label=\"Train\")\n",
    "plt.xlabel(\"$Regularization$\")\n",
    "plt.ylabel(\"$Accuracy$\")\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка качества модели\n",
    "train_error = grid_search.score(X_train, y_train)\n",
    "test_error = grid_search.score(X_test, y_test)\n",
    "\n",
    "print(\"Train Accuracy:\", train_error)\n",
    "print(\"Test Accuracy:\", test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Веса классов\n",
    "plt.figure(figsize=(10,5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.title(f\"class = {grid_search.best_estimator_.classes_[i]}\")\n",
    "    plt.imshow(grid_search.best_estimator_.coef_[i].reshape(-1,8)) #, vmin=-0.2, vmax=0.2)\n",
    "    # plt.colorbar()\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Линейная комбинация изображения и весов\n",
    "plt.figure(figsize=(10,5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    h = grid_search.best_estimator_.coef_[i].reshape(-1,8) * digits.images[IMAGE_INDX]\n",
    "    plt.title(f\"class = {grid_search.best_estimator_.classes_[i]}\\nsum = {h.sum():.2f}\")\n",
    "    plt.imshow(h, vmin=-2, vmax=2)\n",
    "    # plt.colorbar()\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Источники"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1.1. Linear Models](https://scikit-learn.org/stable/modules/linear_model.html)\n",
    "- [3.3. Metrics and scoring: quantifying the quality of predictions](https://scikit-learn.org/stable/modules/model_evaluation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
