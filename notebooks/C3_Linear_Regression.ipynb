{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression\n",
    "\n",
    "<hr>\n",
    "\n",
    "Sergei Yu. Papulin (papulin.study@yandex.ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"0\"></a>\n",
    "<div><span style=\"font-size:16pt; font-weight:bold\">Contents</span>\n",
    "    <ol>\n",
    "        <li><a href=\"#1\">Loading Initial Data</a></li>\n",
    "        <li><a href=\"#2\">Defining Linear Regression Task</a></li>\n",
    "        <li><a href=\"#3\">Brute-Force Search</a></li>\n",
    "        <li><a href=\"#4\">Ordinary Least Squares</a></li>\n",
    "        <li><a href=\"#5\">Gradient Descent</a></li>\n",
    "        <li><a href=\"#6\">Stochastic Gradient Descent</a></li>\n",
    "        <li><a href=\"#7\">Linear Regression in Sklearn</a></li>\n",
    "        <li><a href=\"#8\">References</a></li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules and functions that will be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "<div style=\"display:table; width:100%; padding-top:10px; padding-bottom:10px; border-bottom:1px solid lightgrey\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-size:16pt; font-weight:bold\">1. Loading Initial Data</div>\n",
    "    \t<div style=\"display:table-cell; width:20%; text-align:center; background-color:whitesmoke; border:1px solid lightgrey\"><a href=\"#0\">To contents</a></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe from the csv-file of student grades and show the first 5 records/samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"../data/SAT_GPA.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(FILE_PATH, sep=\" \")\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a `high_GPA`-`univ_GRA` scatter plot for the loaded data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "\n",
    "x_high = df1[\"high_GPA\"].values\n",
    "y_univ = df1[\"univ_GPA\"].values\n",
    "\n",
    "plt.figure(\"1\", figsize=[10, 6])\n",
    "\n",
    "plt.subplot(1,1,1)\n",
    "\n",
    "plt.scatter(x_high, y_univ, color=\"slategrey\")\n",
    "plt.xlabel(\"High_GPA\")\n",
    "plt.ylabel(\"Univ_GPA\")\n",
    "plt.axis([2, 4, 2, 4])\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "# Single Point\n",
    "\n",
    "x_A = df1.loc[36, \"high_GPA\"]\n",
    "y_A = df1.loc[36, \"univ_GPA\"]\n",
    "\n",
    "xy_A = \"$(\"+str(x_A)+\",\"+str(y_A)+\")$\"\n",
    "plt.annotate(xy_A, xy=(x_A, y_A), xytext=(50, -100), xycoords=\"data\", textcoords=\"offset points\", \n",
    "             arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc,angleA=0,armA=0,angleB=-90,armB=15,rad=7\"),)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "<div style=\"display:table; width:100%; padding-top:10px; padding-bottom:10px; border-bottom:1px solid lightgrey\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-size:16pt; font-weight:bold\">2. Defining Linear Regression Task</div>\n",
    "    \t<div style=\"display:table-cell; width:20%; text-align:center; background-color:whitesmoke; border:1px solid lightgrey\"><a href=\"#0\">To contents</a></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(2, 4, num=100)\n",
    "\n",
    "w0 = widgets.FloatSlider(min=-1.5, max=1.2, step=0.05, value=0.75)\n",
    "w1 = widgets.FloatSlider(min=0.4, max=2.0, step=0.05, value=0.75)\n",
    "\n",
    "def update(w0=0, w1=0):\n",
    "   \n",
    "    plt.figure(\"2\", figsize=[10, 6])  \n",
    "    plt.scatter(x_high, y_univ, color=\"slategrey\", label=\"Samples\")\n",
    "    plt.xlabel(\"High_GPA\")\n",
    "    plt.ylabel(\"Univ_GPA\")\n",
    "    plt.plot(x_axis, w1 * x_axis + w0, color=\"darkorange\", linewidth=2, label=\"Regression Line\")\n",
    "    plt.axis([2, 4, 2, 4])\n",
    "    plt.grid(True)\n",
    "    plt.ylim(2, 4)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "widgets.interact(update, w0=w0, w1=w1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider three linear functions and determine which one is the best of then for prediction:\n",
    "\n",
    "1. $h_1\\left( x \\right) = -3 + 2x$\n",
    "\n",
    "2. $h_2\\left( x \\right) = 1.1 + 0.7x$\n",
    "\n",
    "3. $h_3\\left( x \\right) = 2 + 0.4x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize regression parameters of the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_1 = 2.0; slope_2 = 0.7; slope_3 = 0.4\n",
    "intercept_1 = -3.0; intercept_2 = 1.1; intercept_3 = 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create python functions to compute predictions:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pred_1 = lambda x: slope_1 * x + intercept_1\n",
    "h_pred_2 = lambda x: slope_2 * x + intercept_2\n",
    "h_pred_3 = lambda x: slope_3 * x + intercept_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the functions on the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"2\", figsize=[10, 6])\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "\n",
    "# Samples\n",
    "\n",
    "plt.xlabel(\"High_GPA\")\n",
    "plt.ylabel(\"Univ_GPA\")\n",
    "plt.scatter(x_high, y_univ, color=\"slategrey\", label=\"Samples\")\n",
    "\n",
    "\n",
    "# Linear Functions\n",
    "\n",
    "x_line = np.array([1.5, 4.5])\n",
    "plt.plot(x_line, h_pred_1(x_line), \"-\", label=\"$h_1(x)=0.9\\cdot x+0.3$\")\n",
    "plt.plot(x_line, h_pred_2(x_line), \"-\", label=\"$h_2(x)=0.7\\cdot x+1.1$\")\n",
    "plt.plot(x_line, h_pred_3(x_line), \"-\", label=\"$h_3(x)=0.4\\cdot x+2.0$\")\n",
    "\n",
    "\n",
    "plt.axis([2, 4, 2, 4])\n",
    "plt.grid(True)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the functions gives the best prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = h_pred_1(x_high)\n",
    "y_pred_2 = h_pred_2(x_high)\n",
    "y_pred_3 = h_pred_3(x_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_1 = ((y_univ - y_pred_1)**2).sum()\n",
    "err_2 = ((y_univ - y_pred_2)**2).sum()\n",
    "err_3 = ((y_univ - y_pred_3)**2).sum()\n",
    "\n",
    "err_1, err_2, err_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat \\theta_0, \\hat \\theta_1 =\\operatorname*{arg\\,min}_{\\theta_0, \\theta_1} \n",
    "\\displaystyle\\sum_{i=1}^{N} ( y_i - (\\theta_0+\\theta_1 x_i))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "<div style=\"display:table; width:100%; padding-top:10px; padding-bottom:10px; border-bottom:1px solid lightgrey\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-size:16pt; font-weight:bold\">3. Brute-Force Search</div>\n",
    "    \t<div style=\"display:table-cell; width:20%; text-align:center; background-color:whitesmoke; border:1px solid lightgrey\"><a href=\"#0\">To contents</a></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumptions about ranges of parameter values.\n",
    "# Actually we can be far away from real values\n",
    "coord_w0 = np.arange(-4, 5, 0.1)\n",
    "coord_w1 = np.arange(-4, 5, 0.1)\n",
    "\n",
    "W0, W1 = np.meshgrid(coord_w0, coord_w1)\n",
    "W0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_err = np.vectorize(\n",
    "    lambda w0, w1: np.sum((y_univ - (w0 + w1*x_high))**2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs = calculate_err(W0, W1)\n",
    "errs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairs of parameters [(w0, w1),...]\n",
    "W = np.dstack([W0, W1]).reshape(-1, 2)\n",
    "W[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs_alt = np.apply_along_axis(\n",
    "    func1d=lambda w: np.sum((y_univ - (w[0] + w[1]*x_high))**2),\n",
    "    axis=1, \n",
    "    arr=W\n",
    ")\n",
    "errs_alt[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brude force error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_bf = errs.min()\n",
    "\n",
    "err_1, err_2, err_3, err_bf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_bf = W[errs.argmin()]\n",
    "w_bf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pred_bf = lambda x: w_bf[1] * x + w_bf[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"3\", figsize=[10, 6])\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "\n",
    "# Samples\n",
    "\n",
    "plt.scatter(x_high, y_univ, color=\"slategrey\", label=\"Samples\")\n",
    "plt.xlabel(\"High_GPA\")\n",
    "plt.ylabel(\"Univ_GPA\")\n",
    "\n",
    "\n",
    "# Regression Lines\n",
    "\n",
    "x_line = np.array([1.5, 4.5])\n",
    "plt.plot(x_line, h_pred_1(x_line), \"-\", label=\"$f_1(x)=0.9\\cdot x+0.3$\")\n",
    "plt.plot(x_line, h_pred_2(x_line), \"-\", label=\"$f_2(x)=0.7\\cdot x+1.1$\")\n",
    "plt.plot(x_line, h_pred_3(x_line), \"-\", label=\"$f_3(x)=0.4\\cdot x+2.0$\")\n",
    "plt.plot(x_line, h_pred_bf(x_line), \"-\", label=\"$f_{BF}(x)$\")\n",
    "\n",
    "plt.axis([2, 4, 2, 4])\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "<div style=\"display:table; width:100%; padding-top:10px; padding-bottom:10px; border-bottom:1px solid lightgrey\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-size:16pt; font-weight:bold\">4. Ordinary Least Squares</div>\n",
    "    \t<div style=\"display:table-cell; width:20%; text-align:center; background-color:whitesmoke; border:1px solid lightgrey\"><a href=\"#0\">To contents</a></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimates of parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{\\theta}=\\left(\\begin{matrix}{\\hat{\\theta}}_0\\\\\\begin{matrix}{\\hat{\\theta}}_1\\\\\\vdots\\\\\\end{matrix}\\\\{\\hat{\\theta}}_p\\\\\\end{matrix}\\right)=\\left(X^TX\\right)^{-1}X^T\\mathrm{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create $X$ and $\\mathrm{y}$ matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones(x_high.size), x_high]\n",
    "X[:5,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_univ.reshape(-1, 1)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply the formula for calculating regression parameters $\\theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -r 7\n",
    "w = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the `linalg` module has the built-in function for finding the least-squares solution. Use it to get parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -r 7\n",
    "w, residuals, rank, s = np.linalg.lstsq(X, y, rcond=None)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0, w1 = w[0,0], w[1,0]\n",
    "w0, w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the prediction function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pred_ols = lambda x: w1 * x + w0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the graph of the regression line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"3\", figsize=[10, 6])\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "\n",
    "# Samples\n",
    "\n",
    "plt.scatter(x_high, y_univ, color=\"slategrey\", label=\"Samples\")\n",
    "plt.xlabel(\"High_GPA\")\n",
    "plt.ylabel(\"Univ_GPA\")\n",
    "\n",
    "\n",
    "# Regression Lines\n",
    "\n",
    "x_line = np.array([1.5, 4.5])\n",
    "plt.plot(x_line, h_pred_1(x_line), \"-\", label=\"$f_1(x)=0.9\\cdot x+0.3$\")\n",
    "plt.plot(x_line, h_pred_2(x_line), \"-\", label=\"$f_2(x)=0.7\\cdot x+1.1$\")\n",
    "plt.plot(x_line, h_pred_3(x_line), \"-\", label=\"$f_3(x)=0.4\\cdot x+2.0$\")\n",
    "plt.plot(x_line, h_pred_bf(x_line), \"-\", label=\"$f_{BF}(x)$\")\n",
    "plt.plot(x_line, h_pred_ols(x_line), \"-\", linewidth=4, \n",
    "         label=\"$f_{OLS}(x)$\")\n",
    "\n",
    "\n",
    "plt.axis([2, 4, 2, 4])\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_ols = ((h_pred_ols(x_high) - y_univ)**2).sum()\n",
    "err_ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_1, err_2, err_3, err_bf, err_ols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "<div style=\"display:table; width:100%; padding-top:10px; padding-bottom:10px; border-bottom:1px solid lightgrey\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-size:16pt; font-weight:bold\">5. Gradient Descent</div>\n",
    "    \t<div style=\"display:table-cell; width:20%; text-align:center; background-color:whitesmoke; border:1px solid lightgrey\"><a href=\"#0\">To contents</a></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss/cost function and its partial derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of sample elements\n",
    "n = len(x_high)\n",
    "\n",
    "# Loss function \n",
    "loss = lambda x, y, w0, w1: 1 / n * sum([(y[i] - w1 * x[i] - w0) ** 2 for i in range(n)])\n",
    "\n",
    "# Partial derivative of the loss over w0 and w1\n",
    "derivative_w0 = lambda x, y, w0, w1: 2 / n * sum([-1 * (y[i] - w1 * x[i] - w0) for i in range(n)])\n",
    "derivative_w1 = lambda x, y, w0, w1: 2 / n * sum([-x[i] * ( y[i] - w1 * x[i] - w0) for i in range(n)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function implementing the gradient descent method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_descent_(x, y, max_iter=200, min_err=0.0001, learning_rate=0.05):\n",
    "    \"\"\"\n",
    "    Naive Gradient Descent (single feature, row computation)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    learning_rate: step size\n",
    "    max_iter: the maximum numbers of iterations\n",
    "    min_err: a minimal change of cost error\n",
    "    err: a cost function\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    i = 0  # current iteration\n",
    "    w0 = 0; w1 = 0  #  initial parameters\n",
    "    w0_prev = 4; w1_prev = 4  # start point\n",
    "\n",
    "    while i < max_iter:\n",
    "\n",
    "        w0 = w0_prev - learning_rate * derivative_w0(x, y, w0_prev, w1_prev)\n",
    "        w1 = w1_prev - learning_rate * derivative_w1(x, y, w0_prev, w1_prev)\n",
    "\n",
    "        if abs(loss(x, y, w0, w1) - loss(x, y, w0_prev, w1_prev)) <= min_err:\n",
    "            break\n",
    "\n",
    "        w0_prev = w0\n",
    "        w1_prev = w1\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return (w0, w1, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_descent(\n",
    "    X, y, \n",
    "    max_iter=200, \n",
    "    min_err=0.0001, \n",
    "    learning_rate=0.05, \n",
    "    initial_w=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Naive Gradient Descent (multiple features, matrix computation)\n",
    "    \"\"\"\n",
    "    # Number of samples and features\n",
    "    n, p = X.shape\n",
    "    # Initial weights (parameters)\n",
    "    w = np.zeros((p, 1)) if initial_w is None else initial_w.reshape(-1, 1)\n",
    "    # Loss function\n",
    "    loss = 1.0/n * np.sum((X @ w - y)**2)\n",
    "    for i in range(max_iter):\n",
    "        # Compute gradient\n",
    "        gradient = 2.0 / n * X.T @ (X @ w - y)\n",
    "        # Update weights\n",
    "        w -= learning_rate * gradient\n",
    "        # Calculate loss\n",
    "        loss_ = 1.0/n * np.sum((X @ w - y)**2)\n",
    "        # Check stop criteria\n",
    "        if abs(loss - loss_) <= min_err:\n",
    "            break\n",
    "        loss = loss_\n",
    "    return (*w.reshape(-1).tolist(), i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch parameters estimation using GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_gd, w1_gd, i = compute_gradient_descent_(x_high, y_univ)\n",
    "w0_gd, w1_gd, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_gradient_descent(X, y, initial_w=np.array([4., 4.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the prediction function with the estimated parameters:\n",
    "h_pred_gd = lambda x: w1_gd * x + w0_gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the prediction line\n",
    "\n",
    "x_line = np.array([1.5, 4.5])\n",
    "\n",
    "plt.figure(\"3\", figsize=[10, 6])\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "plt.scatter(x_high, y_univ, color=\"slategrey\", label=\"Samples\")\n",
    "plt.plot(x_line, h_pred_2(x_line), \"-\", label=\"$f_2(x)$\")\n",
    "plt.plot(x_line, h_pred_ols(x_line), \"-\", linewidth=4, \n",
    "         label=\"$f_{OLS}(x)$\")\n",
    "plt.plot(x_line, h_pred_gd(x_line), \"-\", linewidth=2, color=\"black\",\n",
    "         label=\"$f_{GD}(x)$\")\n",
    "\n",
    "plt.xlabel(\"High_GPA\")\n",
    "plt.ylabel(\"Univ_GPA\")\n",
    "\n",
    "plt.axis([2, 4, 2, 4])\n",
    "\n",
    "plt.grid(True)\n",
    "ax.set_axisbelow(True) \n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the MSE for the prediction function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_gd = ((y_univ - h_pred_gd(x_high))**2).sum()\n",
    "err_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_1, err_2, err_3, err_bf, err_ols, err_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's wrong? Plot the contour graphic of the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_w0 = np.arange(-4, 5, 0.1)\n",
    "coord_w1 = np.arange(-4, 5, 0.1)\n",
    "\n",
    "W0, W1 = np.meshgrid(coord_w0, coord_w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_w0_large = np.arange(-400, 500, 1)\n",
    "coord_w1_large = np.arange(-400, 500, 1)\n",
    "\n",
    "W0_large, W1_large = np.meshgrid(coord_w0_large, coord_w1_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"12\",figsize=[12, 4])\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.contour(W0, W1, loss(x_high, y_univ, W0, W1), 20, cmap=cm.bwr, alpha=0.5)\n",
    "plt.grid(True)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"$\\\\theta_0$\")\n",
    "plt.ylabel(\"$\\\\theta_1$\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.contour(W0_large, W1_large, loss(x_high, y_univ, W0_large, W1_large), 20, cmap=cm.bwr, alpha=0.5)\n",
    "plt.grid(True)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"$\\\\theta_0$\")\n",
    "plt.ylabel(\"$\\\\theta_1$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ x_s = \\frac{x - \\bar{x}}{s} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization for single feature\n",
    "x_high__stand = (x_high - x_high.mean()) / x_high.std()\n",
    "x_high__stand[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization for feature matrix\n",
    "X_ = x_high.reshape(-1, 1)\n",
    "X_stand = (X_ - X_.mean(axis=0)) / X_.std(axis=0)\n",
    "X_stand = np.c_[np.ones(X_stand.shape[0]), X_stand]\n",
    "X_stand[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = np.array([1.5, 4.5])\n",
    "\n",
    "plt.figure(\"3\", figsize=[12, 4])\n",
    "\n",
    "ax = plt.subplot(1,2,1)\n",
    "\n",
    "plt.scatter(x_high, y_univ, color=\"slategrey\", label=\"Samples\")\n",
    "plt.title(\"Initial\")\n",
    "plt.xlabel(\"High_GPA\")\n",
    "plt.ylabel(\"Univ_GPA\")\n",
    "\n",
    "plt.axis([2, 4, 2, 4])\n",
    "\n",
    "plt.grid(True)\n",
    "ax.set_axisbelow(True) \n",
    "\n",
    "plt.legend()\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "\n",
    "plt.scatter(x_high__stand, y_univ, color=\"slategrey\", label=\"Samples\")\n",
    "plt.title(\"Standardized\")\n",
    "plt.xlabel(\"High_GPA\")\n",
    "plt.ylabel(\"Univ_GPA\")\n",
    "\n",
    "plt.grid(True)\n",
    "ax.set_axisbelow(True) \n",
    "\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"12\", figsize=[12, 4])\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.contour(W0, W1, loss(x_high, y_univ, W0, W1), 20, cmap=cm.bwr, alpha=0.5)\n",
    "plt.title(\"Initial\")\n",
    "plt.xlabel(\"$\\\\theta_0$\")\n",
    "plt.ylabel(\"$\\\\theta_1$\")\n",
    "plt.grid(True)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.contour(W0, W1, loss(x_high__stand, y_univ, W0, W1), 20, cmap=cm.bwr, alpha=0.5)\n",
    "plt.title(\"Standardized\")\n",
    "plt.xlabel(\"$\\\\theta_0$\")\n",
    "plt.ylabel(\"$\\\\theta_1$\")\n",
    "plt.grid(True)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat training with the GD for the standardized feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "w0_gd_stand, w1_gd_stand, i = compute_gradient_descent_(x_high__stand, y_univ)\n",
    "w0_gd_stand, w1_gd_stand, i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the prediction function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pred_gr_stand = lambda x: w1_gd_stand * x + w0_gd_stand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line_stand = np.array([-2, 2])\n",
    "\n",
    "plt.figure(\"3\", figsize=[10, 6])\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "plt.scatter(x_high__stand, y_univ, color=\"slategrey\", label=\"Samples\")\n",
    "plt.plot(x_line_stand, h_pred_gr_stand(x_line_stand), \"-\", linewidth=2, color=\"black\",\n",
    "         label=\"$f_{StndGD}(x)$\")\n",
    "\n",
    "plt.xlabel(\"High_GPA\")\n",
    "plt.ylabel(\"Univ_GPA\")\n",
    "\n",
    "plt.grid(True)\n",
    "ax.set_axisbelow(True) \n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recover the initial scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_gd_rec = w1_gd_stand / x_high.std()\n",
    "w1_gd_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_gd_rec = w0_gd_stand - w1_gd_stand * x_high.mean() / x_high.std()\n",
    "w0_gd_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the prediction function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pred_gr_stand_recover = lambda x: w0_gd_rec + w1_gd_rec * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the prediction line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = np.array([1.5, 4.5])\n",
    "\n",
    "plt.figure(\"3\", figsize=[10, 6])\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "plt.scatter(x_high, y_univ, color=\"slategrey\", label=\"Samples\")\n",
    "plt.plot(x_line, h_pred_2(x_line), \"-\", label=\"$f_2(x)$\")\n",
    "plt.plot(x_line, h_pred_ols(x_line), \"-\", linewidth=4, \n",
    "         label=\"$f_{OLS}(x)$\")\n",
    "plt.plot(x_line, h_pred_gd(x_line), \"-\", linewidth=2, color=\"black\",\n",
    "         label=\"$f_{GD}(x)$\")\n",
    "plt.plot(x_line, h_pred_gr_stand_recover(x_line), \"-\", linewidth=2, color=\"cyan\",\n",
    "         label=\"$f_{StdGD}(x)$\")\n",
    "\n",
    "plt.xlabel(\"High_GPA\")\n",
    "plt.ylabel(\"Univ_GPA\")\n",
    "\n",
    "plt.axis([2, 4, 2, 4])\n",
    "\n",
    "plt.grid(True)\n",
    "ax.set_axisbelow(True) \n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the RSS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_gd_stand = ((y_univ - h_pred_gr_stand_recover(x_high))**2).sum()\n",
    "err_gd_stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_1, err_2, err_3, err_bf, err_ols, err_gd, err_gd_stand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "<div style=\"display:table; width:100%; padding-top:10px; padding-bottom:10px; border-bottom:1px solid lightgrey\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-size:16pt; font-weight:bold\">6. Stochastic Gradient Descent</div>\n",
    "    \t<div style=\"display:table-cell; width:20%; text-align:center; background-color:whitesmoke; border:1px solid lightgrey\"><a href=\"#0\">To contents</a></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss/cost function and its partial derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of sample elements\n",
    "n = len(x_high)\n",
    "\n",
    "# Loss function \n",
    "loss = lambda x, y, w0, w1: 1 / n * sum([(y[i] - w1*x[i] - w0) ** 2 for i in range(n)])\n",
    "\n",
    "# Partial derivative of err over w0 and w1\n",
    "derivative_w0_i = lambda x, y, w0, w1, i: -2.0 * (y[i] - w0 - w1*x[i])\n",
    "derivative_w1_i = lambda x, y, w0, w1, i: -2.0 * x[i] * (y[i] - w0 - w1*x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stochastic_gradient_descent_(x, y, min_err=0.000001, learning_rate=0.05):\n",
    "    \"\"\"\n",
    "    Naive Stochastic Gradient Descent (single feature, row computation)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    learning_rate: step size\n",
    "    min_err: a minimal change of cost error\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w0: intercept\n",
    "    w1: slope\n",
    "    i: the number of iteration\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    i = 0  # current iteration\n",
    "    w0 = 0; w1 = 0  #  initial parameters\n",
    "    w0_prev = 4; w1_prev = 4  # start point\n",
    "    \n",
    "    err_prev = loss(x, y, w0, w1)\n",
    "    err_cur = err_prev\n",
    "\n",
    "    for j in range(n):\n",
    "    \n",
    "        w0 = w0_prev - learning_rate * derivative_w0_i(x, y, w0_prev, w1_prev, j)\n",
    "        w1 = w1_prev - learning_rate * derivative_w1_i(x, y, w0_prev, w1_prev, j)\n",
    "        \n",
    "        err_cur = loss(x, y, w0, w1)\n",
    "\n",
    "        if abs(err_cur - err_prev) <= min_err:\n",
    "                break\n",
    "\n",
    "        err_prev = err_cur    \n",
    "\n",
    "        w0_prev = w0\n",
    "        w1_prev = w1\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return (w0, w1, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stochastic_gradient_descent(\n",
    "    X, y, \n",
    "    learning_rate=0.05, \n",
    "    min_err=0.000001,\n",
    "    num_epochs=1, \n",
    "    batch_size=1,\n",
    "    initial_w=None\n",
    "):\n",
    "    # Number of samples and features\n",
    "    n, p = X.shape\n",
    "    # Initial weights (parameters)\n",
    "    w = np.zeros((p, 1)) if initial_w is None else initial_w.reshape(-1, 1)\n",
    "    # Loss function\n",
    "    loss = 1.0/n * np.sum((X @ w - y)**2)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle data \n",
    "        shuffled_indices = np.random.permutation(n)        \n",
    "        X_shuffled = X[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "        # Iterate over mini-batches\n",
    "        for i in range(0, n, batch_size):\n",
    "            # Compose mini-batch\n",
    "            X_batch = X_shuffled[i:i + batch_size]\n",
    "            y_batch = y_shuffled[i:i + batch_size]\n",
    "            # Compute gradient\n",
    "            gradient = 2.0 / batch_size * X_batch.T @ (X_batch @ w - y_batch)\n",
    "            # Update weights\n",
    "            w -= learning_rate * gradient\n",
    "        # Calculate loss\n",
    "        loss_ = 1.0/n * np.sum((X @ w - y)**2)\n",
    "        # Check stop criteria\n",
    "        if abs(loss - loss_) <= min_err:\n",
    "            break\n",
    "        loss = loss_\n",
    "    return (*w.reshape(-1).tolist(), epoch*n + i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate regression parameters using the SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "w0_sgd_stand, w1_sgd_stand, i = compute_stochastic_gradient_descent_(x_high__stand, y_univ)\n",
    "w0_sgd_stand, w1_sgd_stand, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_sgd_stand, w1_sgd_stand, i = compute_stochastic_gradient_descent(X_stand, y)\n",
    "w0_sgd_stand, w1_sgd_stand, i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the prediction function with the found parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pred_sgd = lambda x: w1_sgd_stand * x + w0_sgd_stand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the prediction line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line_stand = np.array([-2, 2])\n",
    "\n",
    "plt.figure(\"3\", figsize=[10, 6])\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "plt.scatter(x_high__stand, y_univ, color=\"slategrey\", label=\"Samples\")\n",
    "plt.plot(x_line_stand, h_pred_sgd(x_line_stand), \"-\", linewidth=2, color=\"black\",\n",
    "         label=\"$f_{StndGD}(x)$\")\n",
    "\n",
    "plt.xlabel(\"High_GPA\")\n",
    "plt.ylabel(\"Univ_GPA\")\n",
    "\n",
    "plt.grid(True)\n",
    "ax.set_axisbelow(True) \n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recover the initial scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_sgd_rec = w1_sgd_stand / x_high.std()\n",
    "w1_sgd_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_sgd_rec = w0_sgd_stand - w1_sgd_stand * x_high.mean() / x_high.std()\n",
    "w0_sgd_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the prediction function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pred_sgd_stand_recover = lambda x: w0_sgd_rec + w1_sgd_rec * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the prediction line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = np.array([1.5, 4.5])\n",
    "\n",
    "plt.figure(\"3\", figsize=[10, 6])\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "plt.scatter(x_high, y_univ, color=\"slategrey\", label=\"Samples\")\n",
    "plt.plot(x_line, h_pred_ols(x_line), \"-\", linewidth=4, \n",
    "         label=\"$h_{OLS}(x)$\")\n",
    "plt.plot(x_line, h_pred_gr_stand_recover(x_line), \"-\", linewidth=1, color=\"cyan\",\n",
    "         label=\"$h_{StdGD}(x)$\")\n",
    "plt.plot(x_line, h_pred_sgd_stand_recover(x_line), \"-\", linewidth=4, color=\"darkmagenta\",\n",
    "         label=\"$h_{StdSGD}(x)$\")\n",
    "\n",
    "plt.xlabel(\"High_GPA\")\n",
    "plt.ylabel(\"Univ_GPA\")\n",
    "\n",
    "plt.axis([2, 4, 2, 4])\n",
    "\n",
    "plt.grid(True)\n",
    "ax.set_axisbelow(True) \n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_sgd_stand = ((y_univ - h_pred_sgd_stand_recover(x_high))**2).sum()\n",
    "err_sgd_stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_1, err_2, err_3, err_bf, err_ols, err_gd, err_gd_stand, err_sgd_stand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"7\"></a>\n",
    "<div style=\"display:table; width:100%; padding-top:10px; padding-bottom:10px; border-bottom:1px solid lightgrey\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-size:16pt; font-weight:bold\">7. Linear Regression in Sklearn</div>\n",
    "    \t<div style=\"display:table-cell; width:20%; text-align:center; background-color:whitesmoke; border:1px solid lightgrey\"><a href=\"#0\">To contents</a></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1[[\"high_GPA\"]]\n",
    "y = df1[\"univ_GPA\"]\n",
    "\n",
    "# X = df1[[\"high_GPA\"]].values\n",
    "# y = df1[\"univ_GPA\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X, y)\n",
    "model.intercept_, model.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_sklearn__ols = np.sum((y - model.predict(X))**2)\n",
    "err_sklearn__ols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = df1[\"high_GPA\"].mean()\n",
    "std = df1[\"high_GPA\"].std()\n",
    "\n",
    "X_stand = (df1[[\"high_GPA\"]] - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDRegressor().fit(X_stand, y)\n",
    "\n",
    "# Parameters for standardized X\n",
    "model.intercept_, model.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_sklearn__sgd = np.sum((y - model.predict(X_stand))**2)\n",
    "err_sklearn__sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"8\"></a>\n",
    "<div style=\"display:table; width:100%; padding-top:10px; padding-bottom:10px; border-bottom:1px solid lightgrey\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-size:16pt; font-weight:bold\">8. References</div>\n",
    "    \t<div style=\"display:table-cell; width:20%; text-align:center; background-color:whitesmoke; border:1px solid lightgrey\"><a href=\"#0\">To contents</a></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
