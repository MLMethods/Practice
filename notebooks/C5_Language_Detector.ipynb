{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Распознавание языка текста\n",
    "\n",
    "<hr>\n",
    "\n",
    "С.Ю. Папулин (papulin.study@yandex.ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Содержание\n",
    "\n",
    "- [Статический текст](#Статический-текст)\n",
    "- [Динамический текст](#Динамический-текст)\n",
    "    - [Построение модели](#Построение-модели)\n",
    "    - [Проверка динамического распознавания](#Проверка-динамического-распознавания)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подключение библиотек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Статический текст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Набор данных](https://huggingface.co/datasets/papluca/language-identification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "20 Languages Dataset:\n",
    "https://huggingface.co/datasets/papluca/language-identification\n",
    "\"\"\"\n",
    "\n",
    "from os import makedirs, remove\n",
    "from os.path import exists, join\n",
    "import gzip\n",
    "\n",
    "from sklearn.datasets.base import RemoteFileMetadata, _fetch_remote\n",
    "from sklearn.datasets import get_data_home\n",
    "from sklearn.utils import Bunch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "ARCHIVES = [\n",
    "    RemoteFileMetadata(\n",
    "        filename='languages_train.csv',\n",
    "        url='https://huggingface.co/datasets/papluca/language-identification/resolve/main/train.csv',\n",
    "        checksum=('f180d78a1f0e758fd33bb1bae37f62eebc538d78ece2affb3d05a967850ba474')),\n",
    "    RemoteFileMetadata(\n",
    "        filename='languages_test.csv',\n",
    "        url='https://huggingface.co/datasets/papluca/language-identification/resolve/main/test.csv',\n",
    "        checksum=('cb7dfe272142815573b735b5d555d42d28d0d648187020f2d2eb3eebd772e759'))\n",
    "   \n",
    "]\n",
    "\n",
    "\n",
    "def fetch_languages(data_home=None, download_if_missing=True, subset='all', return_X_y=False):  \n",
    "    data_home = get_data_home(data_home=data_home)\n",
    "    if not exists(data_home):\n",
    "        makedirs(data_home)\n",
    "    for archive in ARCHIVES:\n",
    "        filepath = join(data_home, archive.filename)\n",
    "        if not exists(filepath):\n",
    "            if not download_if_missing:\n",
    "                raise IOError(\"Data not found and `download_if_missing` is False\")\n",
    "            logger.info('Downloading Languages from {} to {}'.format(\n",
    "                archive.url, filepath))\n",
    "            archive_path = _fetch_remote(archive, dirname=data_home)\n",
    "    if return_X_y:\n",
    "        DESCR = (\n",
    "            '20 Languages Dataset\\n'\n",
    "            '--------------------\\n'\n",
    "            'The Language Identification dataset is a collection of 90k samples consisting of text passages and corresponding language label. This dataset was created by collecting data from 3 sources: [Multilingual Amazon Reviews Corpus](https://huggingface.co/datasets/amazon_reviews_multi), [XNLI](https://huggingface.co/datasets/xnli), and [STSb Multi MT](https://huggingface.co/datasets/stsb_multi_mt).\\n'\n",
    "            '\\n'\n",
    "            'The Language Identification dataset contains text in 20 languages, which are:\\n'\n",
    "            'arabic (ar), bulgarian (bg), german (de), modern greek (el), english (en), spanish (es), french (fr), hindi (hi), italian (it), japanese (ja), dutch (nl), polish (pl), portuguese (pt), russian (ru), swahili (sw), thai (th), turkish (tr), urdu (ur), vietnamese (vi), and chinese (zh)\\n'\n",
    "            '\\n'\n",
    "            'For each instance, there is a string for the text and a string for the label (the language tag). Here is an example:\\n'\n",
    "            \"{'labels': 'fr', 'text': 'Conforme à la description, produit pratique.'}\"\n",
    "        )\n",
    "        if subset == 'train':\n",
    "            train_df = _load_X_y(data_home, 'train')\n",
    "            return Bunch(\n",
    "                data=train_df,\n",
    "                DESCR=DESCR\n",
    "            )\n",
    "        elif subset == 'test':\n",
    "            test_df = _load_X_y(data_home, 'test')\n",
    "            return Bunch(\n",
    "                data=test_df,\n",
    "                DESCR=DESCR\n",
    "            )\n",
    "        train_df = _load_X_y(data_home, 'train')\n",
    "        test_df = _load_X_y(data_home, 'test')\n",
    "        return Bunch(\n",
    "            data={'train': train_df, 'test': test_df},\n",
    "            DESCR=DESCR\n",
    "        )\n",
    "\n",
    "\n",
    "def _load_X_y(path, subset='train'):\n",
    "    return pd.read_csv(join(path, 'languages_{}.csv'.format(subset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "dataset = fetch_languages(return_X_y=True)\n",
    "\n",
    "# Вывод описания\n",
    "print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.data['train']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../data/lang_detector/train.csv')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Количество текстов по каждому классу\n",
    "df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Среднее количество символов в текстах по каждому классу\n",
    "df.groupby('labels').agg(\n",
    "    lambda group: group['text'].str.len().mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(df['text'], df['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../data/lang_detector/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(df_test['text'], df_test['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Динамический текст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Исходные данные\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Шаблон для делителя строки на слова\n",
    "COMPILER = re.compile(\"\\W+\", re.UNICODE)\n",
    "\n",
    "\n",
    "def split_sentence(lang, text):\n",
    "    s = list()\n",
    "    for word in set(COMPILER.split(text)):\n",
    "        if word:\n",
    "            s.append((lang, word))\n",
    "    return s\n",
    "\n",
    "\n",
    "# Формирование списка пар язык-слово\n",
    "data = list()\n",
    "for i, row in df.iterrows():\n",
    "    data += split_sentence(row['labels'], row['text'])\n",
    "    \n",
    "    \n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Формивание датафрейма язык-слово и удаление повторений\n",
    "df_new = pd.DataFrame(data=data, columns=['labels', 'word']).drop_duplicates()\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = 'обуч'\n",
    "\n",
    "print(\n",
    "    df_new[df_new['word'].str.contains(INPUT)]\\\n",
    "        .groupby('labels')\\\n",
    "        .count().T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Априорные вероятности классов\n",
    "# class_prior=[\n",
    "#     0.04, 0.04, 0.05, 0.05, 0.1, 0.05, 0.05, 0.04, 0.05, 0.05,\n",
    "#     0.05, 0.05, 0.05, 0.05, 0.05, 0.04, 0.04, 0.05, 0.05, 0.05\n",
    "# ]\n",
    "# class_prior=[0.05]*20\n",
    "\n",
    "# Пострение модели классификации\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(analyzer='char', ngram_range=(2,4))),\n",
    "    ('classifier', MultinomialNB(class_prior=class_prior))\n",
    "])\n",
    "\n",
    "# Обучение модели\n",
    "pipeline.fit(df_new['word'], df_new['labels'])\n",
    "\n",
    "print(\n",
    "    # Оценка качества на тестовом множестве (из первой задачи)\n",
    "    f\"Accuracy = { pipeline.score(df_test['text'], df_test['labels']) }\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.named_steps['vectorizer'].vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Классы\n",
    "langs = pipeline.named_steps['classifier'].classes_\n",
    "langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = 'обуч'\n",
    "\n",
    "# Вероятности принадлежности классам для некоторого слова\n",
    "probs = pipeline.predict_proba([INPUT,])[0]\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка динамического распознавания "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_prediction(langs, probs):\n",
    "    \"\"\"\n",
    "    Отображение вероятностей по языкам \n",
    "    в виде датафрейма.\n",
    "    \"\"\"\n",
    "    print(\n",
    "        pd.DataFrame(\n",
    "            data=zip(langs, probs),\n",
    "            columns=['lang', 'prob']\n",
    "        )\\\n",
    "        .sort_values('prob', ascending=0)\\\n",
    "        .head(10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ввод текста\n",
    "text_input = widgets.Text()\n",
    "display(text_input)\n",
    "\n",
    "# Вывод результата предсказания\n",
    "output = widgets.Output()\n",
    "display(output)\n",
    "\n",
    "\n",
    "def handle_process_text(sender):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        probs = pipeline.predict_proba([sender.new,])[0]\n",
    "        langs = pipeline.named_steps['classifier'].classes_\n",
    "        display_prediction(langs, probs)\n",
    "\n",
    "\n",
    "# Отслеживание ввода\n",
    "text_input.observe(handle_process_text, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
